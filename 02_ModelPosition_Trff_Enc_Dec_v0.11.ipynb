{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers, constraints\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Utils\n",
    "\n",
    "class Experiment(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__ = kwargs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    \n",
    "def euclideanDistance(xp, x, yp, y):\n",
    "    return tf.reduce_mean(tf.math.sqrt(tf.square(xp - x) + tf.square(yp - y)))\n",
    "\n",
    "def meanAbsoluteError(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def meanSquaredError(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    \n",
    "def customLossFunction(floor, x, y, p_floor, p_x, p_y):\n",
    "    penalty_ = tf.constant(15.0, dtype=tf.float32)\n",
    "    \n",
    "    p_floor = tf.math.round(p_floor)\n",
    "    \n",
    "    ed_ = euclideanDistance(p_x, x, p_y, y)\n",
    "    f_ = tf.abs(floor - p_floor)\n",
    "    loss_ = ed_ + penalty_ * f_\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def normalize(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "def denormalize(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "\n",
    "def plotMetrics(history, figsize=(12, 8)):\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00b430a79c947fc9e2de9ffa5aed848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=24464.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eab552de344a42a6a450da992a4675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26925.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a851585bd534f12b4769039a83f504e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=24464.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209efb46a075413e93129f2a57ab8d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.034499672988881624\n",
      "23620 844 24464 24464\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 1. Experiment & Global Variables & Paths\n",
    "\n",
    "# Paths \n",
    "\n",
    "PATH_DATA = '../../01_Data/'\n",
    "PATH_MODELS = '../../03_Models/'\n",
    "PATH_PREDICTIONS = '../../04_Predictions/'\n",
    "\n",
    "path_data_train = PATH_DATA + 'train/'\n",
    "path_data_test = PATH_DATA + 'test/'\n",
    "path_metadata = PATH_DATA + 'metadata/'\n",
    "\n",
    "path_generated_data = PATH_DATA + 'GeneratedData/v0.2/'\n",
    "path_generated_data_train = path_generated_data + 'train/'\n",
    "path_generated_data_test = path_generated_data + 'test/'\n",
    "path_output_data_images = PATH_DATA + 'GeneratedData/v0.2/images/'\n",
    "\n",
    "# Global Variables\n",
    "\n",
    "DICT_FLOOR_MAP = {'1F' :  0, '2F' : 1, '3F' : 2, '4F' : 3, '5F' : 4, \n",
    "                     '6F' : 5, '7F' : 6, '8F' : 7, '9F' : 8,\n",
    "                     'B'  : -1, 'B1' : -1, 'B2' : -2, 'B3' : -3, \n",
    "                     'BF' : -1, 'BM' : -1, \n",
    "                     'F1' : 0, 'F2' : 1, 'F3' : 2, 'F4' : 3, 'F5' : 4, \n",
    "                     'F6' : 5, 'F7' : 6, 'F8' : 7, 'F9' : 8, 'F10': 9,\n",
    "                     'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3, 'L5' : 4, \n",
    "                     'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8, 'L10': 9, \n",
    "                     'L11': 10,\n",
    "                     'G'  : 0, 'LG1': 0, 'LG2': 1, 'LM' : 0, 'M'  : 0, \n",
    "                     'P1' : 0, 'P2' : 1,}\n",
    "\n",
    "VERSION = '009'\n",
    "\n",
    "list_train_paths = glob.glob(path_generated_data_train + '*')\n",
    "list_test_paths = glob.glob(path_generated_data_test + '*')\n",
    "list_metadata_paths = glob.glob(path_metadata + '*')\n",
    "\n",
    "list_train_traces = [trace.split('\\\\')[-1] for trace in list_train_paths]\n",
    "list_test_traces = [trace.split('\\\\')[-1] for trace in list_test_paths]\n",
    "\n",
    "assert len(list_train_paths)==len(list_train_traces)\n",
    "assert len(list_test_paths)==len(list_test_traces)\n",
    "\n",
    "dict_train_traces_paths = {trace : path for trace, path in zip(list_train_traces, list_train_paths)}\n",
    "dict_test_traces_paths = {trace : path for trace, path in zip(list_test_traces, list_test_paths)}\n",
    "\n",
    "# Load experiment\n",
    "with open(f'{path_generated_data}Experiment_{VERSION}.pkl', 'rb') as f:\n",
    "    experiment = pickle.load(f)\n",
    "    \n",
    "print(f'Version: {experiment.version}')\n",
    "\n",
    "\n",
    "#Embedding dims\n",
    "\n",
    "EMB_SITE_DIM = len(experiment.dict_all_sites)\n",
    "EMB_UUID_DIM = len(experiment.dict_unique_uuid)\n",
    "EMB_BSSID_DIM = len(experiment.dict_unique_bssid)\n",
    "EMB_SSID_DIM = len(experiment.dict_unique_ssid)\n",
    "\n",
    "# Floors\n",
    "\n",
    "dict_training_floor = {}\n",
    "for trace in tqdm(list_train_traces):\n",
    "    path = dict_train_traces_paths[trace]\n",
    "    data = np.load(f'{path}/waypoint_predict.npy', allow_pickle=True).astype(np.float32)\n",
    "    floor = data[:, 0][0]\n",
    "    dict_training_floor[trace] = floor\n",
    "    \n",
    "    \n",
    "# Trace + Site + Floor\n",
    "dict_site_floor_traces = {}\n",
    "list_site_floor_traces = glob.glob(PATH_DATA  + 'train/' + '/*/*/*.txt')\n",
    "for file in tqdm(list_site_floor_traces):\n",
    "    list_file = file.split('\\\\')\n",
    "    site = list_file[1]\n",
    "    floor = list_file[2]\n",
    "    trace = list_file[-1].replace('.txt', '')\n",
    "    dict_site_floor_traces[trace] = (site, floor)\n",
    "    \n",
    "    \n",
    "# For each site, floor how many traces there are\n",
    "dict_train_site_floor_traces = {}\n",
    "for trace in tqdm(dict_train_traces_paths):\n",
    "    labels = np.load(f'{dict_train_traces_paths[trace]}/waypoint_predict.npy', allow_pickle=True).astype(np.float32)\n",
    "    site, floor = dict_site_floor_traces[trace]\n",
    "    if site not in dict_train_site_floor_traces:\n",
    "        dict_train_site_floor_traces[site] = {\n",
    "            floor : [trace]\n",
    "        }\n",
    "    elif floor not in dict_train_site_floor_traces[site]:\n",
    "        dict_train_site_floor_traces[site][floor] = [trace]\n",
    "    else:\n",
    "        dict_train_site_floor_traces[site][floor].extend([trace])\n",
    "        \n",
    "# build train val trraces        \n",
    "list_train_train_traces, list_train_val_traces = [], []\n",
    "for site in tqdm(dict_train_site_floor_traces):\n",
    "    num_floors_site = len(dict_train_site_floor_traces[site])\n",
    "    for floor in dict_train_site_floor_traces[site]:\n",
    "        if num_floors_site==1:\n",
    "            list_traces = dict_train_site_floor_traces[site][floor]\n",
    "            list_train_train_traces.extend(list_traces)\n",
    "        else:\n",
    "            list_traces = dict_train_site_floor_traces[site][floor]\n",
    "            np.random.shuffle(list_traces)\n",
    "            num_train = int(len(list_traces) * 0.95) + 1\n",
    "            list_train_train_traces.extend(list_traces[:num_train])\n",
    "            list_train_val_traces.extend(list_traces[num_train:])\n",
    "            \n",
    "            \n",
    "pc_val_traces = len(list_train_val_traces) / (len(list_train_val_traces)+len(list_train_train_traces))\n",
    "print(pc_val_traces)\n",
    "print(len(list_train_train_traces), len(list_train_val_traces), \n",
    "      len(list_train_train_traces) + len(list_train_val_traces), len(list_train_traces))\n",
    "\n",
    "# MIN_FLOOR = min(experiment.list_floors_train_filtered)\n",
    "# MAX_FLOOR = max(experiment.list_floors_train_filtered)\n",
    "NUM_FLOORS = len(experiment.list_floors_train_filtered)\n",
    "# DICT_MAP_FLOORS = {f : i for i, f in enumerate(experiment.list_floors_train_filtered)}\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 3. Functions\n",
    "\n",
    "def getAngles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positionalEncoding(position, trf_dim):\n",
    "    angle_rads = getAngles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(trf_dim)[np.newaxis, :],\n",
    "                           trf_dim)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 2. Data Generator\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, list_traces, dict_traces_paths, batch_size,\n",
    "                 trf_dim=128, experiment=None, shuffle=True, training=True, dict_floors=None):\n",
    "        super(DataGenerator, self).__init__()\n",
    "        self.list_traces = list_traces\n",
    "        self.dict_traces_paths = dict_traces_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_floors = dict_floors\n",
    "        self.experiment = experiment\n",
    "        self.trf_dim = trf_dim\n",
    "        self.training = training\n",
    "        self.shuffle = shuffle\n",
    "        self.pad_w = 107\n",
    "        self.pad_num_w = 1_000\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.list_traces) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_trace = [self.list_traces[k] for k in indexes]\n",
    "        \n",
    "        if self.training:\n",
    "            l_arr_imu, l_arr_wifi_beacon, l_arr_wdata, l_arr_labels = [], [], [], []\n",
    "            l_train_x, l_train_y, l_pad_mask = [], [], []\n",
    "            for trace in list_batch_trace:\n",
    "                imu = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/seq_imu.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                wifi_beacon = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/seq_wifi_beacon.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                wdata = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/waypoint_data.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                labels = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/waypoint_predict.npy', allow_pickle=True).astype(np.float32), 0) \n",
    "                \n",
    "                site, floor = wdata[:, 0, 0][0], labels[:, 0, 0][0]\n",
    "                \n",
    "                arr_floor = np.expand_dims(np.repeat(np.asarray([[floor + 2]]), wdata.shape[1], axis=1), -1)\n",
    "                wdata = np.concatenate([arr_floor, wdata], axis=-1)\n",
    "                num_w = wdata.shape[1]\n",
    "                labels[:, :, 1:] /= 100.0\n",
    "\n",
    "                padt = (0, (self.pad_w-imu.shape[1]))\n",
    "                arr_imu = np.pad(imu, ((0, 0), padt, (0, 0), (0, 0)), constant_values=(0))\n",
    "                arr_wifi_beacon = np.pad(wifi_beacon, ((0, 0), padt, (0, 0), (0, 0)), constant_values=(0))\n",
    "                arr_wdata = np.pad(wdata, ((0, 0), padt, (0, 0)), constant_values=(0))\n",
    "                arr_labels = np.pad(labels, ((0, 0), padt, (0, 0)), constant_values=(-1))\n",
    "                \n",
    "                arr_pad_mask = np.where(np.equal(arr_wdata[:, :, 1:], 0), 0, 1)\n",
    "                arr_pad_mask[:, 0, :] = 1.\n",
    "                \n",
    "                arr_imu = (arr_imu - self.experiment.mean_imu)/self.experiment.std_imu\n",
    "                arr_wdata[:, :, 2:] = (arr_wdata[:, :, 2:] - self.experiment.mean_w)/self.experiment.std_w\n",
    "                arr_wifi_beacon[:, :, :, 2:6] = (arr_wifi_beacon[:, :, :, 2:6] - self.experiment.mean_wifi)/self.experiment.std_wifi\n",
    "                arr_wifi_beacon[:, :, :, 7:] = (arr_wifi_beacon[:, :, :, 7:] - self.experiment.mean_beacon)/self.experiment.std_beacon\n",
    "                \n",
    "                \n",
    "                l_arr_imu.append(arr_imu)\n",
    "                l_arr_wifi_beacon.append(arr_wifi_beacon)\n",
    "                l_arr_wdata.append(arr_wdata)\n",
    "                l_arr_labels.append(arr_labels)\n",
    "                l_pad_mask.append(arr_pad_mask)\n",
    "\n",
    "            x_imu = np.concatenate(l_arr_imu)\n",
    "            x_wb = np.concatenate(l_arr_wifi_beacon)\n",
    "            x_wdata = np.concatenate(l_arr_wdata)\n",
    "\n",
    "            y = np.concatenate(l_arr_labels)[:, :, 1:]\n",
    "            pad_mask = np.concatenate(l_pad_mask).astype(np.float32)\n",
    "            enc_pad_mask, combined_mask, _ = self.createMasks(pad_mask, x_wdata)\n",
    "            pos_encoding = positionalEncoding(self.pad_w, self.trf_dim)\n",
    "            # pos_encoding_beacon = \n",
    "            pos_encoding_timestamp = np.repeat(np.expand_dims(x_wdata[:, :, 3], -1), self.trf_dim, -1)\n",
    "            pos_encoding_imu = np.expand_dims(positionalEncoding(128, self.trf_dim), 1)\n",
    "            x_wifi = x_wb[:, :, :100, :6]\n",
    "            x_beacon = x_wb[:, :, :100, 6:]\n",
    "            return (x_imu, x_wifi, x_beacon, x_wdata, enc_pad_mask, pos_encoding, combined_mask), y\n",
    "        \n",
    "        else:\n",
    "            l_arr_imu, l_arr_wifi_beacon, l_arr_wdata, l_pad_mask = [], [], [], []\n",
    "            for trace in list_batch_trace:\n",
    "                imu = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/seq_imu.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                wifi_beacon = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/seq_wifi_beacon.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                wdata = np.expand_dims(np.load(f'{self.dict_traces_paths[trace]}/waypoint_data.npy', allow_pickle=True).astype(np.float32), 0)\n",
    "                \n",
    "                site, floor = wdata[:, 0, 0][0], self.dict_floors[trace]\n",
    "                arr_floor = np.expand_dims(np.repeat(np.asarray([[floor + 2]]), wdata.shape[1], axis=1), -1)\n",
    "                wdata = np.concatenate([arr_floor, wdata], axis=-1)\n",
    "                padt = (0, (self.pad_w-imu.shape[1]))\n",
    "                arr_imu = np.pad(imu, ((0, 0), padt, (0, 0), (0, 0)), constant_values=(0))\n",
    "                arr_wifi_beacon = np.pad(wifi_beacon, ((0, 0), padt, (0, 0), (0, 0)), constant_values=(0))\n",
    "                arr_wdata = np.pad(wdata, ((0, 0), padt, (0, 0)), constant_values=(0))\n",
    "                \n",
    "                arr_pad_mask = np.where(np.equal(arr_wdata[:, :, 1:], 0), 0, 1)\n",
    "                arr_pad_mask[:, 0, :] = 1.\n",
    "                arr_imu = (arr_imu - self.experiment.mean_imu)/self.experiment.std_imu\n",
    "                arr_wdata[:, :, 2:] = (arr_wdata[:, :, 2:] - self.experiment.mean_w)/self.experiment.std_w\n",
    "                arr_wifi_beacon[:, :, :, 2:6] = (arr_wifi_beacon[:, :, :, 2:6] - self.experiment.mean_wifi)/self.experiment.std_wifi\n",
    "                arr_wifi_beacon[:, :, :, 7:] = (arr_wifi_beacon[:, :, :, 7:] - self.experiment.mean_beacon)/self.experiment.std_beacon\n",
    "\n",
    "                l_arr_imu.append(arr_imu)\n",
    "                l_arr_wifi_beacon.append(arr_wifi_beacon)\n",
    "                l_arr_wdata.append(arr_wdata)\n",
    "                l_pad_mask.append(arr_pad_mask)\n",
    "                \n",
    "            x_imu = np.concatenate(l_arr_imu)\n",
    "            x_wb = np.concatenate(l_arr_wifi_beacon)\n",
    "            x_wdata = np.concatenate(l_arr_wdata)\n",
    "            pad_mask = np.concatenate(l_pad_mask).astype(np.float32)\n",
    "            enc_pad_mask, combined_mask, _ = self.createMasks(pad_mask, x_wdata)\n",
    "            pos_encoding = positionalEncoding(self.pad_w, self.trf_dim)\n",
    "            pos_encoding_imu = np.expand_dims(positionalEncoding(128, self.trf_dim), 1)\n",
    "            pos_encoding_timestamp = np.repeat(np.expand_dims(x_wdata[:, :, 3], -1), self.trf_dim, -1)\n",
    "            x_wifi = x_wb[:, :, :100, :6]\n",
    "            x_beacon = x_wb[:, :, :100, 6:]\n",
    "            return (x_imu, x_wifi, x_beacon, x_wdata, pad_mask, pos_encoding, combined_mask)\n",
    "            \n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_traces))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.list_traces)\n",
    "            \n",
    "    def createPaddingMask(self, x):\n",
    "        mask = tf.cast(tf.math.equal(x, 1), tf.bool)\n",
    "        # add extra dimensions to add the padding to the attention logits.\n",
    "        # (batch_size, 1, 1, sequence length)\n",
    "        mask = mask[:, tf.newaxis, tf.newaxis, :, 0]\n",
    "        return mask\n",
    "    \n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "            \n",
    "    def createMasks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = self.createPaddingMask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = self.createPaddingMask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by \n",
    "        # the decoder.\n",
    "        #look_ahead_mask = self.createLookAheadMask(tf.shape(tar)[1])\n",
    "        #dec_target_padding_mask = self.createPaddingMask(tar)\n",
    "        #combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        combined_mask = self.causal_attention_mask(tf.shape(tar)[0], tf.shape(tar)[1], tf.shape(tar)[1], tf.bool)\n",
    "\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "            \n",
    "            \n",
    "    def createLookAheadMask(self, seq_len):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen = DataGenerator(list_train_traces, dict_train_traces_paths, batch_size=8,\n",
    "#                         experiment=experiment, \n",
    "#                         shuffle=True, training=True)\n",
    "# for batch in datagen:\n",
    "#     break\n",
    "\n",
    "# # x_imu, x_wifi, x_beacon, x_wdata, enc_pad_mask, pos_encoding, combined_mask\n",
    "# in_imu, in_wifi, in_beacon, in_w, in_enc_pad_mask, in_pos_encoding, in_combined_mask, in_target = \\\n",
    "#             batch[0][0], batch[0][1], batch[0][2], batch[0][3], batch[0][4], batch[0][5], batch[0][6], batch[1]\n",
    "\n",
    "# print(in_imu.shape, in_wifi.shape, in_beacon.shape, in_w.shape, )\n",
    "# print(in_enc_pad_mask.shape, in_pos_encoding.shape, in_combined_mask.shape)\n",
    "# print(in_target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 4. Models\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes)\n",
    "        self.ffn = models.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "\n",
    "class ProcessEncoderInput(layers.Layer):\n",
    "    def __init__(self, trf_dim, hide_dim=50):\n",
    "        super(ProcessEncoderInput, self).__init__()\n",
    "        \n",
    "        # Class Embedings\n",
    "        self.emb_ssid = layers.Embedding(EMB_SSID_DIM, 60, mask_zero=True, name='emb_ssid')\n",
    "        self.emb_bssid = layers.Embedding(EMB_BSSID_DIM, 120, mask_zero=True, name='emb_bssid')\n",
    "        self.emb_uuid = layers.Embedding(EMB_UUID_DIM, 60, mask_zero=True, name='emb_uuiid')\n",
    "        self.emb_site = layers.Embedding(EMB_SITE_DIM, 20, mask_zero=False, name='site_embedding')\n",
    "        self.emb_floor = layers.Embedding(NUM_FLOORS, 3, mask_zero=False, name='floor_embedding')\n",
    "        \n",
    "        self.encoder_imu = EncoderTransformerBlock(trf_dim, num_heads, trf_dim*4)\n",
    "        self.encoder_wifi = EncoderTransformerBlock(trf_dim, num_heads, trf_dim*4)\n",
    "        self.encoder_beacon = EncoderTransformerBlock(trf_dim, num_heads, trf_dim*4)\n",
    "        # self.encoder_waypoint = EncoderTransformerBlock(trf_dim, num_heads, trf_dim*4)\n",
    "        \n",
    "        # Continous features\n",
    "        self.d_imu = layers.Dense(trf_dim)\n",
    "        self.d_wifi_beacon = layers.Dense(trf_dim)\n",
    "        self.d_way = layers.Dense(trf_dim)\n",
    "        self.l_norm = layers.LayerNormalization()\n",
    "        # self.d_all = layers.Dense(trf_dim)\n",
    "        self.drop1 = layers.Dropout(0.2)\n",
    "        self.drop2 = layers.Dropout(0.2)\n",
    "        self.drop3 = layers.Dropout(0.2)\n",
    "        self.drop4 = layers.Dropout(0.2)\n",
    "        \n",
    "        # memories\n",
    "        self.seq_imu = models.Sequential([\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(64, 'relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(trf_dim),\n",
    "            layers.LayerNormalization()\n",
    "        ])\n",
    "        \n",
    "        self.seq_wifi = models.Sequential([\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(trf_dim*3, 'relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(trf_dim),\n",
    "            layers.LayerNormalization()\n",
    "        ])\n",
    "        \n",
    "        self.seq_beacon = models.Sequential([\n",
    "            # layers.Dropout(0.3),\n",
    "            #layers.Dense(trf_dim, 'relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(trf_dim),\n",
    "            layers.LayerNormalization()\n",
    "        ])\n",
    "        \n",
    "#         self.seq_way = models.Sequential([\n",
    "#             layers.Dropout(0.3),\n",
    "#             layers.Dense(20, 'relu'),\n",
    "#             layers.Dropout(0.2),\n",
    "#             layers.Dense(trf_dim),\n",
    "#             layers.LayerNormalization()\n",
    "#         ])\n",
    "    \n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        in_imu, in_wifi, in_beacon, in_waypoint, pos_encoding, enc_pad_mask = inputs\n",
    "        \n",
    "        seq_len = tf.shape(in_waypoint)[1]\n",
    "        # Entities\n",
    "        x_ssid = self.emb_ssid(in_wifi[:, :, :, 0])\n",
    "        x_bssid = self.emb_bssid(in_wifi[:, :, :, 1])\n",
    "        x_uuid = self.emb_uuid(in_beacon[:, :, :, 0])\n",
    "        x_floor = self.emb_floor(in_waypoint[:, :, 0])\n",
    "        x_site = self.emb_site(in_waypoint[:, :, 1])\n",
    "        \n",
    "        x_imu = tf.reshape(in_imu, [tf.shape(in_imu)[0], tf.shape(in_imu)[1], tf.shape(in_imu)[2]*tf.shape(in_imu)[3]])\n",
    "        x_imu = self.seq_imu(x_imu)\n",
    "        \n",
    "        x_wifi = tf.concat([x_ssid, x_bssid, in_wifi[:, :, :, 2:6]], axis=-1)\n",
    "        x_wifi = tf.reshape(x_wifi, [tf.shape(x_wifi)[0], tf.shape(x_wifi)[1], \n",
    "                                                 tf.shape(x_wifi)[2]*tf.shape(x_wifi)[3]])\n",
    "        x_wifi = self.seq_wifi(x_wifi)\n",
    "        \n",
    "        x_beacon = tf.concat([x_uuid, in_beacon[:, :, :, 1:]], axis=-1)\n",
    "        x_beacon = tf.reshape(x_beacon, [tf.shape(x_beacon)[0], tf.shape(x_beacon)[1], \n",
    "                                         tf.shape(x_beacon)[2]*tf.shape(x_beacon)[3]])\n",
    "        x_beacon = self.seq_beacon(x_beacon)\n",
    "        \n",
    "        x_way = tf.concat([x_floor, x_site, in_waypoint[:, :, 2:]], axis=-1)\n",
    "        # x_way = self.seq_way(x_way)\n",
    "        \n",
    "        # x = x_imu + x_wifi + x_beacon + x_way + pos_encoding\n",
    "        x_imu = self.encoder_imu(x_imu, x_imu, training=training, attention_mask=enc_pad_mask)\n",
    "        x_wifi = self.encoder_wifi(x_wifi, x_wifi, training=training, attention_mask=enc_pad_mask)\n",
    "        x_beacon = self.encoder_beacon(x_beacon, x_beacon, training=training, attention_mask=enc_pad_mask)\n",
    "        # x_way = self.encoder_waypoint(x_way, x_way, training=training, attention_mask=enc_pad_mask)\n",
    "        \n",
    "        x =  x_imu + x_wifi + x_beacon# + x_way\n",
    "        x = self.l_norm(x)\n",
    "        \n",
    "        return x, x_way\n",
    "    \n",
    "    \n",
    "    \n",
    "class Encoder(models.Model):\n",
    "    def __init__(self, trf_dim=64, num_heads=2, ff_dim=100, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.d_dim = layers.Dense(trf_dim)\n",
    "        self.dropout = layers.Dropout(0.1)\n",
    "        self.list_transformer_blocks = [EncoderTransformerBlock(trf_dim, num_heads, ff_dim)\n",
    "                                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.gru = layers.GRU(trf_dim, return_sequences=True, dropout=0.1)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        enc_input, pad_mask, pos_encoding, in_combined_mask = inputs\n",
    "        seq_len = tf.shape(enc_input)[1]\n",
    "\n",
    "        x = self.dropout(enc_input, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.list_transformer_blocks[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class TransformerModel(models.Model):\n",
    "    \n",
    "    def __init__(self, hide_dim, trf_dim, num_heads, ff_dim, num_enc_layers, max_seq_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.trf_dim = trf_dim\n",
    "        self.proc_encoder = ProcessEncoderInput(trf_dim=trf_dim, hide_dim=hide_dim)\n",
    "        self.encoder = Encoder(trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4, num_layers=num_enc_layers)\n",
    "        ###\n",
    "        self.gru = layers.Bidirectional(layers.GRU(trf_dim//4, return_sequences=True, dropout=0.3))\n",
    "        self.drop_out_1 = layers.Dropout(0.1)\n",
    "        self.drop_out_2 = layers.Dropout(0.1)\n",
    "        self.dense_out1 = layers.Dense(32, activation='relu')\n",
    "        self.dense_out2 = layers.Dense(2, activation='relu')\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        ###\n",
    "        in_imu, in_wifi, in_beacon, in_way, in_enc_pad_mask, pos_encoding, in_combined_mask = inputs\n",
    "        seq_len = tf.shape(in_way)[1]\n",
    "        ### \n",
    "        ###\n",
    "        p_enc_out, x_sf = self.proc_encoder((in_imu, in_wifi, in_beacon, in_way, pos_encoding, in_enc_pad_mask), training=training)\n",
    "        \n",
    "        enc_output = self.encoder((p_enc_out, in_enc_pad_mask, pos_encoding, in_combined_mask), \n",
    "                                  training=training)\n",
    "        # print(f'Enc output: {enc_output.shape}')\n",
    "        out = self.gru(enc_output, \n",
    "                       training=training, \n",
    "                       mask=tf.cast(in_enc_pad_mask[:, 0, 0, :seq_len], tf.bool))\n",
    "\n",
    "        out = self.drop_out_1(out, training=training)\n",
    "        out = tf.concat([out, x_sf], axis=-1)\n",
    "        out = self.dense_out1(out)\n",
    "        # out = self.drop_out_2(out, training=training)\n",
    "        out = self.dense_out2(out)  \n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "def getTransformerModel(hide_dim, trf_dim, num_heads, ff_dim, num_enc_layers, max_seq_len):\n",
    "    return TransformerModel(hide_dim, trf_dim, num_heads, ff_dim, num_enc_layers, max_seq_len)\n",
    "\n",
    "\n",
    "def lossFunction(y_real, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_real, -1))\n",
    "    weights = tf.concat([tf.ones((1, 1, 1)) * 2.0, tf.ones((1, tf.shape(y_real)[1]-1, 1))], axis=1)\n",
    "    loss_ = tf.math.sqrt(tf.expand_dims(tf.square(tf.math.subtract(y_real[:, :, 0], y_pred[:, :, 0])), -1) + \\\n",
    "                         tf.expand_dims(tf.square(tf.math.subtract(y_real[:, :, 1], y_pred[:, :, 1])), -1))\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    mask = tf.expand_dims(tf.reduce_max(mask, axis=-1), -1)\n",
    "    loss_ *= mask\n",
    "    loss_ *= weights\n",
    "    \n",
    "    return  tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def cMetric(y_real, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_real, -1))\n",
    "    loss_ = tf.math.sqrt(tf.expand_dims(tf.square(tf.math.subtract(y_real[:, :, 0], y_pred[:, :, 0])), -1) + \\\n",
    "                         tf.expand_dims(tf.square(tf.math.subtract(y_real[:, :, 1], y_pred[:, :, 1])), -1))\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    mask = tf.expand_dims(tf.reduce_max(mask, axis=-1), -1)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return  tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def maskedMAE(y_real, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_real, -1))\n",
    "    metric = tf.math.abs(tf.math.subtract(y_real, y_pred))\n",
    "\n",
    "    mask = tf.cast(mask, dtype=metric.dtype)\n",
    "    metric *= mask\n",
    "\n",
    "    return tf.reduce_sum(metric)/tf.reduce_sum(mask)\n",
    "\n",
    "def maskedMAEStart(y_real, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_real[:, 0, :], -1))\n",
    "    metric = tf.math.abs(tf.math.subtract(y_real[:, 0, :], y_pred[:, 0, :]))\n",
    "\n",
    "    mask = tf.cast(mask, dtype=metric.dtype)\n",
    "    metric *= mask\n",
    "\n",
    "    return tf.reduce_sum(metric)/tf.reduce_sum(mask)\n",
    "    \n",
    "def plotMetrics(history, figsize=(12, 8)):\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(history.history['maskedMAE'])\n",
    "    plt.plot(history.history['val_maskedMAE'])\n",
    "    plt.title('Model Masked MAE')\n",
    "    plt.ylabel('maskedMAE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            \n",
    "            \n",
    "class CustomSchedule(optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "                  \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 20 and epoch % 5 == 0:\n",
    "        return lr*0.9\n",
    "    else:\n",
    "        return lr\n",
    "                                   \n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def createPaddingMask(x):\n",
    "#     mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "#     # add extra dimensions to add the padding to the attention logits.\n",
    "#     # (batch_size, 1, 1, sequence length)\n",
    "#     mask = mask[:, tf.newaxis, tf.newaxis, :, 0]\n",
    "#     return mask\n",
    "\n",
    "# datagen = DataGenerator(list_train_traces, dict_train_traces_paths, batch_size=8, trf_dim=128,\n",
    "#                                      experiment=experiment, shuffle=True, training=True)\n",
    "# for batch in datagen:\n",
    "#     break\n",
    "\n",
    "# print(len(batch[0]), len(batch[0][0]), len(batch[0][1]), len(batch[0][2]), len(batch[1]))\n",
    "# print(batch[0][0].shape, batch[0][1].shape, batch[0][2].shape, batch[1].shape)\n",
    "\n",
    "# in_imu, in_wifibeacon, in_w, in_shift_target, in_enc_pad_mask, in_dec_pad_mask, in_look_ahead_mask, \\\n",
    "#          in_pos_encoding, in_target = \\\n",
    "#             batch[0][0], batch[0][1], batch[0][2], batch[0][3], batch[0][4], batch[0][5], \\\n",
    "#             batch[0][6], batch[0][7], batch[1]\n",
    "\n",
    "# print(in_imu.shape, in_wifibeacon.shape, in_w.shape, in_shift_target.shape, in_enc_pad_mask.shape, in_dec_pad_mask.shape,\n",
    "#       in_look_ahead_mask.shape, in_pos_encoding.shape, in_target.shape)\n",
    "\n",
    "\n",
    "# hide_dim = 32\n",
    "# trf_dim = 128\n",
    "# num_heads = 2\n",
    "\n",
    "# pos_encoding = np.zeros((1, 107, trf_dim))\n",
    "# enc_pad_mask = createPaddingMask(in_enc_pad_mask)\n",
    "# dec_pad_mask = createPaddingMask(in_dec_pad_mask)\n",
    "\n",
    "# p_enc = ProcessEncoderInput(trf_dim=trf_dim, hide_dim=hide_dim)\n",
    "# p_enc_out = p_enc((in_imu, in_wifibeacon))\n",
    "# print(f'ProcessEncoderInput out: {p_enc_out.shape}')\n",
    "\n",
    "# enc = Encoder(trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4)\n",
    "# enc_output = enc((p_enc_out, enc_pad_mask, in_pos_encoding))\n",
    "# print(f'Encoder out: {enc_output.shape}')\n",
    "\n",
    "# p_dec = ProcessDecoderInput(trf_dim=trf_dim)\n",
    "# p_dec_out = p_dec((in_w, in_shift_target))\n",
    "# print(f'ProcessDecoderInput out: {p_dec_out.shape}')\n",
    "\n",
    "# dec = Decoder(trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4)\n",
    "# dec_output = dec((enc_output, p_dec_out, in_pos_encoding, dec_pad_mask, in_look_ahead_mask))\n",
    "# print(f'Decoder: {dec_output.shape}')\n",
    "\n",
    "# m = TransformerModel(hide_dim=hide_dim, trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4)\n",
    "# m_output = m((in_imu, in_wifibeacon, in_w, in_shift_target, in_enc_pad_mask, in_dec_pad_mask, in_look_ahead_mask))\n",
    "# print(f'Transformer model: {m_output.shape}')\n",
    "\n",
    "# 7 8 8 8 8\n",
    "# (8, 107, 128, 23) (8, 107, 128, 8) (8, 107, 3) (8, 107, 2)\n",
    "# (8, 107, 128, 23) (8, 107, 128, 8) (8, 107, 3) (8, 107, 2) (8, 107, 2) (8, 107, 2) (8, 107, 107) (8, 107, 2)\n",
    "# ProcessEncoderInput out: (8, 107, 15872)\n",
    "# Encoder out: (8, 107, 128)\n",
    "# ProcessDecoderInput out: (8, 107, 128)\n",
    "# Decoder: (8, 107, 128)\n",
    "# Transformer model: (8, 107, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_generator = DataGenerator(['5dc23ce006f7170006addcc0'], dict_train_traces_paths, batch_size=1, \n",
    "#                                    experiment=experiment, shuffle=False, training=True)\n",
    "\n",
    "# for batch in val_data_generator:\n",
    "#     break\n",
    "    \n",
    "# predictions = model(batch[0], training=False)\n",
    "# target = batch[1]\n",
    "\n",
    "# print(target)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train paths: 23620, Num val paths: 844\n",
      "Epoch 1/100\n",
      "370/370 [==============================] - 220s 570ms/step - loss: 0.7131 - cMetric: 0.7138 - maskedMAE: 0.4528 - maskedMAEStart: 0.4316 - val_loss: 0.2805 - val_cMetric: 0.2810 - val_maskedMAE: 0.1778 - val_maskedMAEStart: 0.1785\n",
      "Epoch 2/100\n",
      "370/370 [==============================] - 210s 566ms/step - loss: 0.2662 - cMetric: 0.2662 - maskedMAE: 0.1690 - maskedMAEStart: 0.1621 - val_loss: 0.1970 - val_cMetric: 0.2108 - val_maskedMAE: 0.1342 - val_maskedMAEStart: 0.1237\n",
      "Epoch 3/100\n",
      "370/370 [==============================] - 208s 560ms/step - loss: 0.2010 - cMetric: 0.2010 - maskedMAE: 0.1278 - maskedMAEStart: 0.1208 - val_loss: 0.1885 - val_cMetric: 0.1873 - val_maskedMAE: 0.1179 - val_maskedMAEStart: 0.1112\n",
      "Epoch 4/100\n",
      "370/370 [==============================] - 214s 577ms/step - loss: 0.1672 - cMetric: 0.1671 - maskedMAE: 0.1062 - maskedMAEStart: 0.1039 - val_loss: 0.1571 - val_cMetric: 0.1575 - val_maskedMAE: 0.1001 - val_maskedMAEStart: 0.0982\n",
      "Epoch 5/100\n",
      "370/370 [==============================] - 211s 569ms/step - loss: 0.1487 - cMetric: 0.1486 - maskedMAE: 0.0944 - maskedMAEStart: 0.0926 - val_loss: 0.1476 - val_cMetric: 0.1500 - val_maskedMAE: 0.0949 - val_maskedMAEStart: 0.0876\n",
      "Epoch 6/100\n",
      "370/370 [==============================] - 212s 572ms/step - loss: 0.1359 - cMetric: 0.1359 - maskedMAE: 0.0863 - maskedMAEStart: 0.0845 - val_loss: 0.1297 - val_cMetric: 0.1286 - val_maskedMAE: 0.0814 - val_maskedMAEStart: 0.0846\n",
      "Epoch 7/100\n",
      "370/370 [==============================] - 207s 559ms/step - loss: 0.1263 - cMetric: 0.1266 - maskedMAE: 0.0804 - maskedMAEStart: 0.0797 - val_loss: 0.1268 - val_cMetric: 0.1280 - val_maskedMAE: 0.0815 - val_maskedMAEStart: 0.0776\n",
      "Epoch 8/100\n",
      "370/370 [==============================] - 205s 553ms/step - loss: 0.1182 - cMetric: 0.1182 - maskedMAE: 0.0752 - maskedMAEStart: 0.0746 - val_loss: 0.1246 - val_cMetric: 0.1235 - val_maskedMAE: 0.0787 - val_maskedMAEStart: 0.0885\n",
      "Epoch 9/100\n",
      "370/370 [==============================] - 205s 554ms/step - loss: 0.1128 - cMetric: 0.1127 - maskedMAE: 0.0716 - maskedMAEStart: 0.0714 - val_loss: 0.1132 - val_cMetric: 0.1125 - val_maskedMAE: 0.0710 - val_maskedMAEStart: 0.0736\n",
      "Epoch 10/100\n",
      "370/370 [==============================] - 208s 560ms/step - loss: 0.1073 - cMetric: 0.1074 - maskedMAE: 0.0684 - maskedMAEStart: 0.0674 - val_loss: 0.1135 - val_cMetric: 0.1151 - val_maskedMAE: 0.0731 - val_maskedMAEStart: 0.0764\n",
      "Epoch 11/100\n",
      "370/370 [==============================] - 209s 564ms/step - loss: 0.1024 - cMetric: 0.1023 - maskedMAE: 0.0649 - maskedMAEStart: 0.0644 - val_loss: 0.1134 - val_cMetric: 0.1143 - val_maskedMAE: 0.0725 - val_maskedMAEStart: 0.0714\n",
      "Epoch 12/100\n",
      "370/370 [==============================] - 211s 569ms/step - loss: 0.1011 - cMetric: 0.1011 - maskedMAE: 0.0643 - maskedMAEStart: 0.0632 - val_loss: 0.1098 - val_cMetric: 0.1093 - val_maskedMAE: 0.0693 - val_maskedMAEStart: 0.0707\n",
      "Epoch 13/100\n",
      "370/370 [==============================] - 210s 566ms/step - loss: 0.0960 - cMetric: 0.0960 - maskedMAE: 0.0609 - maskedMAEStart: 0.0604 - val_loss: 0.1046 - val_cMetric: 0.1055 - val_maskedMAE: 0.0672 - val_maskedMAEStart: 0.0703\n",
      "Epoch 14/100\n",
      "370/370 [==============================] - 211s 569ms/step - loss: 0.0933 - cMetric: 0.0931 - maskedMAE: 0.0591 - maskedMAEStart: 0.0582 - val_loss: 0.1001 - val_cMetric: 0.0992 - val_maskedMAE: 0.0628 - val_maskedMAEStart: 0.0651\n",
      "Epoch 15/100\n",
      "370/370 [==============================] - 213s 574ms/step - loss: 0.0904 - cMetric: 0.0904 - maskedMAE: 0.0574 - maskedMAEStart: 0.0573 - val_loss: 0.0990 - val_cMetric: 0.0989 - val_maskedMAE: 0.0629 - val_maskedMAEStart: 0.0658\n",
      "Epoch 16/100\n",
      "370/370 [==============================] - 216s 584ms/step - loss: 0.0883 - cMetric: 0.0882 - maskedMAE: 0.0561 - maskedMAEStart: 0.0554 - val_loss: 0.0989 - val_cMetric: 0.0989 - val_maskedMAE: 0.0629 - val_maskedMAEStart: 0.0675\n",
      "Epoch 17/100\n",
      "370/370 [==============================] - 212s 573ms/step - loss: 0.0864 - cMetric: 0.0864 - maskedMAE: 0.0549 - maskedMAEStart: 0.0544 - val_loss: 0.0998 - val_cMetric: 0.0994 - val_maskedMAE: 0.0632 - val_maskedMAEStart: 0.0625\n",
      "Epoch 18/100\n",
      "370/370 [==============================] - 213s 574ms/step - loss: 0.0837 - cMetric: 0.0837 - maskedMAE: 0.0532 - maskedMAEStart: 0.0521 - val_loss: 0.1015 - val_cMetric: 0.1008 - val_maskedMAE: 0.0643 - val_maskedMAEStart: 0.0650\n",
      "Epoch 19/100\n",
      "370/370 [==============================] - 216s 584ms/step - loss: 0.0831 - cMetric: 0.0837 - maskedMAE: 0.0532 - maskedMAEStart: 0.0518 - val_loss: 0.0978 - val_cMetric: 0.0979 - val_maskedMAE: 0.0617 - val_maskedMAEStart: 0.0652\n",
      "Epoch 20/100\n",
      "370/370 [==============================] - 214s 577ms/step - loss: 0.0806 - cMetric: 0.0806 - maskedMAE: 0.0512 - maskedMAEStart: 0.0502 - val_loss: 0.0979 - val_cMetric: 0.1021 - val_maskedMAE: 0.0649 - val_maskedMAEStart: 0.0642\n",
      "Epoch 21/100\n",
      "370/370 [==============================] - 216s 583ms/step - loss: 0.0795 - cMetric: 0.0796 - maskedMAE: 0.0505 - maskedMAEStart: 0.0497 - val_loss: 0.1028 - val_cMetric: 0.1012 - val_maskedMAE: 0.0644 - val_maskedMAEStart: 0.0639\n",
      "Epoch 22/100\n",
      "370/370 [==============================] - 218s 589ms/step - loss: 0.0774 - cMetric: 0.0774 - maskedMAE: 0.0491 - maskedMAEStart: 0.0487 - val_loss: 0.1084 - val_cMetric: 0.1083 - val_maskedMAE: 0.0686 - val_maskedMAEStart: 0.0709\n",
      "Epoch 23/100\n",
      "370/370 [==============================] - 263s 710ms/step - loss: 0.0754 - cMetric: 0.0753 - maskedMAE: 0.0478 - maskedMAEStart: 0.0479 - val_loss: 0.0985 - val_cMetric: 0.0976 - val_maskedMAE: 0.0617 - val_maskedMAEStart: 0.0620\n",
      "Epoch 24/100\n",
      "370/370 [==============================] - 408s 1s/step - loss: 0.0745 - cMetric: 0.0745 - maskedMAE: 0.0473 - maskedMAEStart: 0.0471 - val_loss: 0.0909 - val_cMetric: 0.0910 - val_maskedMAE: 0.0581 - val_maskedMAEStart: 0.0595\n",
      "Epoch 25/100\n",
      "370/370 [==============================] - 336s 907ms/step - loss: 0.0720 - cMetric: 0.0720 - maskedMAE: 0.0457 - maskedMAEStart: 0.0457 - val_loss: 0.0915 - val_cMetric: 0.0910 - val_maskedMAE: 0.0580 - val_maskedMAEStart: 0.0588\n",
      "Epoch 26/100\n",
      "370/370 [==============================] - 652s 2s/step - loss: 0.0722 - cMetric: 0.0722 - maskedMAE: 0.0459 - maskedMAEStart: 0.0452 - val_loss: 0.0899 - val_cMetric: 0.0885 - val_maskedMAE: 0.0561 - val_maskedMAEStart: 0.0578\n",
      "Epoch 27/100\n",
      "370/370 [==============================] - 664s 2s/step - loss: 0.0698 - cMetric: 0.0698 - maskedMAE: 0.0444 - maskedMAEStart: 0.0442 - val_loss: 0.0901 - val_cMetric: 0.0890 - val_maskedMAE: 0.0565 - val_maskedMAEStart: 0.0602\n",
      "Epoch 28/100\n",
      "370/370 [==============================] - 667s 2s/step - loss: 0.0695 - cMetric: 0.0695 - maskedMAE: 0.0441 - maskedMAEStart: 0.0440 - val_loss: 0.0918 - val_cMetric: 0.0906 - val_maskedMAE: 0.0574 - val_maskedMAEStart: 0.0601\n",
      "Epoch 29/100\n",
      "370/370 [==============================] - 549s 1s/step - loss: 0.0688 - cMetric: 0.0687 - maskedMAE: 0.0436 - maskedMAEStart: 0.0432 - val_loss: 0.0994 - val_cMetric: 0.1001 - val_maskedMAE: 0.0638 - val_maskedMAEStart: 0.0667\n",
      "Epoch 30/100\n",
      "370/370 [==============================] - 205s 553ms/step - loss: 0.0681 - cMetric: 0.0679 - maskedMAE: 0.0431 - maskedMAEStart: 0.0433 - val_loss: 0.0908 - val_cMetric: 0.0892 - val_maskedMAE: 0.0564 - val_maskedMAEStart: 0.0602\n",
      "Epoch 31/100\n",
      "370/370 [==============================] - 204s 551ms/step - loss: 0.0654 - cMetric: 0.0653 - maskedMAE: 0.0415 - maskedMAEStart: 0.0418 - val_loss: 0.0891 - val_cMetric: 0.0887 - val_maskedMAE: 0.0559 - val_maskedMAEStart: 0.0567\n",
      "Epoch 32/100\n",
      "370/370 [==============================] - 207s 559ms/step - loss: 0.0664 - cMetric: 0.0663 - maskedMAE: 0.0421 - maskedMAEStart: 0.0421 - val_loss: 0.0931 - val_cMetric: 0.0924 - val_maskedMAE: 0.0582 - val_maskedMAEStart: 0.0612\n",
      "Epoch 33/100\n",
      "370/370 [==============================] - 204s 552ms/step - loss: 0.0640 - cMetric: 0.0640 - maskedMAE: 0.0406 - maskedMAEStart: 0.0412 - val_loss: 0.0931 - val_cMetric: 0.0946 - val_maskedMAE: 0.0599 - val_maskedMAEStart: 0.0610\n",
      "Epoch 34/100\n",
      "370/370 [==============================] - 204s 551ms/step - loss: 0.0637 - cMetric: 0.0637 - maskedMAE: 0.0404 - maskedMAEStart: 0.0405 - val_loss: 0.0906 - val_cMetric: 0.0943 - val_maskedMAE: 0.0603 - val_maskedMAEStart: 0.0609\n",
      "Epoch 35/100\n",
      "370/370 [==============================] - 204s 551ms/step - loss: 0.0631 - cMetric: 0.0631 - maskedMAE: 0.0401 - maskedMAEStart: 0.0402 - val_loss: 0.0885 - val_cMetric: 0.0891 - val_maskedMAE: 0.0561 - val_maskedMAEStart: 0.0570\n",
      "Epoch 36/100\n",
      "370/370 [==============================] - 204s 552ms/step - loss: 0.0624 - cMetric: 0.0624 - maskedMAE: 0.0396 - maskedMAEStart: 0.0398 - val_loss: 0.0931 - val_cMetric: 0.0919 - val_maskedMAE: 0.0574 - val_maskedMAEStart: 0.0579\n",
      "Epoch 37/100\n",
      "370/370 [==============================] - 206s 555ms/step - loss: 0.0625 - cMetric: 0.0625 - maskedMAE: 0.0396 - maskedMAEStart: 0.0392 - val_loss: 0.0876 - val_cMetric: 0.0865 - val_maskedMAE: 0.0547 - val_maskedMAEStart: 0.0567\n",
      "Epoch 38/100\n",
      "370/370 [==============================] - 210s 568ms/step - loss: 0.0594 - cMetric: 0.0594 - maskedMAE: 0.0377 - maskedMAEStart: 0.0376 - val_loss: 0.0889 - val_cMetric: 0.0885 - val_maskedMAE: 0.0562 - val_maskedMAEStart: 0.0573\n",
      "Epoch 39/100\n",
      "370/370 [==============================] - 209s 565ms/step - loss: 0.0597 - cMetric: 0.0597 - maskedMAE: 0.0379 - maskedMAEStart: 0.0384 - val_loss: 0.0887 - val_cMetric: 0.0881 - val_maskedMAE: 0.0556 - val_maskedMAEStart: 0.0570\n",
      "Epoch 40/100\n",
      "370/370 [==============================] - 209s 565ms/step - loss: 0.0596 - cMetric: 0.0596 - maskedMAE: 0.0378 - maskedMAEStart: 0.0380 - val_loss: 0.0863 - val_cMetric: 0.0860 - val_maskedMAE: 0.0546 - val_maskedMAEStart: 0.0593\n",
      "Epoch 41/100\n",
      "370/370 [==============================] - 207s 560ms/step - loss: 0.0595 - cMetric: 0.0596 - maskedMAE: 0.0378 - maskedMAEStart: 0.0382 - val_loss: 0.0881 - val_cMetric: 0.0876 - val_maskedMAE: 0.0556 - val_maskedMAEStart: 0.0591\n",
      "Epoch 42/100\n",
      "370/370 [==============================] - 207s 560ms/step - loss: 0.0574 - cMetric: 0.0574 - maskedMAE: 0.0364 - maskedMAEStart: 0.0367 - val_loss: 0.0853 - val_cMetric: 0.0846 - val_maskedMAE: 0.0538 - val_maskedMAEStart: 0.0562\n",
      "Epoch 43/100\n",
      "370/370 [==============================] - 209s 565ms/step - loss: 0.0565 - cMetric: 0.0565 - maskedMAE: 0.0359 - maskedMAEStart: 0.0360 - val_loss: 0.0830 - val_cMetric: 0.0827 - val_maskedMAE: 0.0521 - val_maskedMAEStart: 0.0553\n",
      "Epoch 44/100\n",
      "370/370 [==============================] - 209s 563ms/step - loss: 0.0571 - cMetric: 0.0570 - maskedMAE: 0.0362 - maskedMAEStart: 0.0365 - val_loss: 0.0846 - val_cMetric: 0.0848 - val_maskedMAE: 0.0542 - val_maskedMAEStart: 0.0569\n",
      "Epoch 45/100\n",
      "370/370 [==============================] - 214s 578ms/step - loss: 0.0550 - cMetric: 0.0550 - maskedMAE: 0.0349 - maskedMAEStart: 0.0351 - val_loss: 0.0833 - val_cMetric: 0.0843 - val_maskedMAE: 0.0534 - val_maskedMAEStart: 0.0566\n",
      "Epoch 46/100\n",
      "370/370 [==============================] - 214s 578ms/step - loss: 0.0558 - cMetric: 0.0558 - maskedMAE: 0.0354 - maskedMAEStart: 0.0357 - val_loss: 0.0841 - val_cMetric: 0.0841 - val_maskedMAE: 0.0534 - val_maskedMAEStart: 0.0557\n",
      "Epoch 47/100\n",
      "370/370 [==============================] - 213s 574ms/step - loss: 0.0547 - cMetric: 0.0547 - maskedMAE: 0.0347 - maskedMAEStart: 0.0350 - val_loss: 0.0897 - val_cMetric: 0.0889 - val_maskedMAE: 0.0565 - val_maskedMAEStart: 0.0578\n",
      "Epoch 48/100\n",
      "370/370 [==============================] - 214s 578ms/step - loss: 0.0543 - cMetric: 0.0543 - maskedMAE: 0.0344 - maskedMAEStart: 0.0346 - val_loss: 0.0841 - val_cMetric: 0.0861 - val_maskedMAE: 0.0544 - val_maskedMAEStart: 0.0553\n",
      "Epoch 49/100\n",
      " 24/370 [>.............................] - ETA: 3:17 - loss: 0.0555 - cMetric: 0.0555 - maskedMAE: 0.0353 - maskedMAEStart: 0.0350"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-417128e27491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     verbose=1)\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mplotMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 4. Model training\n",
    "\n",
    "callback_early_stopping = ReturnBestEarlyStopping(monitor='val_cMetric', \n",
    "                                                  patience=20, verbose=1, restore_best_weights=True)\n",
    "callback_lrsched = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "list_callbacks = [callback_early_stopping, callback_lrsched]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "hide_dim = 30\n",
    "trf_dim = 128\n",
    "num_enc_layers = 1\n",
    "num_heads = 2\n",
    "batch_size = 64\n",
    "val_size = 0.1\n",
    "model_name = 'trf_enc_dec_v0.13'\n",
    "\n",
    "print(f'Num train paths: {len(list_train_train_traces)}, Num val paths: {len(list_train_val_traces)}')\n",
    "assert len(list_train_train_traces)+len(list_train_val_traces)==len(list_train_traces)\n",
    "\n",
    "train_data_generator = DataGenerator(list_train_train_traces, dict_train_traces_paths, batch_size=batch_size,\n",
    "                                     trf_dim=trf_dim, experiment=experiment, shuffle=True, training=True)\n",
    "\n",
    "val_data_generator = DataGenerator(list_train_val_traces, dict_train_traces_paths, batch_size=batch_size, \n",
    "                                   trf_dim=trf_dim, experiment=experiment, shuffle=True, training=True)\n",
    "\n",
    "# Model\n",
    "\n",
    "model = getTransformerModel(hide_dim=hide_dim, trf_dim=trf_dim, num_heads=num_heads, ff_dim=trf_dim*4,\n",
    "                            num_enc_layers=num_enc_layers, max_seq_len=107)\n",
    "# Loads the weights\n",
    "# model.load_weights(PATH_MODELS + 'position_model/' + model_name + '_weights')\n",
    "learning_rate = 1e-3\n",
    "# learning_rate = CustomSchedule(trf_dim)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                        epsilon=1e-9), \n",
    "              loss=cMetric,\n",
    "              metrics=[cMetric, maskedMAE, maskedMAEStart])\n",
    "\n",
    "history = model.fit(train_data_generator,\n",
    "                    validation_data=val_data_generator,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=list_callbacks,\n",
    "                    epochs=100,\n",
    "                    verbose=1)\n",
    "\n",
    "plotMetrics(history, figsize=(10, 6)) \n",
    "model.save(PATH_MODELS + 'position_model/' + model_name, include_optimizer=False)\n",
    "model.save_weights(PATH_MODELS + 'position_model/' + model_name + '_weights', save_format='tf')\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num train paths: 23620, Num val paths: 844\n",
    "# Epoch 1/100\n",
    "# 370/370 [==============================] - 220s 570ms/step - loss: 0.7131 - cMetric: 0.7138 - maskedMAE: 0.4528 - maskedMAEStart: 0.4316 - val_loss: 0.2805 - val_cMetric: 0.2810 - val_maskedMAE: 0.1778 - val_maskedMAEStart: 0.1785\n",
    "# Epoch 2/100\n",
    "# 370/370 [==============================] - 210s 566ms/step - loss: 0.2662 - cMetric: 0.2662 - maskedMAE: 0.1690 - maskedMAEStart: 0.1621 - val_loss: 0.1970 - val_cMetric: 0.2108 - val_maskedMAE: 0.1342 - val_maskedMAEStart: 0.1237\n",
    "# Epoch 3/100\n",
    "# 370/370 [==============================] - 208s 560ms/step - loss: 0.2010 - cMetric: 0.2010 - maskedMAE: 0.1278 - maskedMAEStart: 0.1208 - val_loss: 0.1885 - val_cMetric: 0.1873 - val_maskedMAE: 0.1179 - val_maskedMAEStart: 0.1112\n",
    "# Epoch 4/100\n",
    "# 370/370 [==============================] - 214s 577ms/step - loss: 0.1672 - cMetric: 0.1671 - maskedMAE: 0.1062 - maskedMAEStart: 0.1039 - val_loss: 0.1571 - val_cMetric: 0.1575 - val_maskedMAE: 0.1001 - val_maskedMAEStart: 0.0982\n",
    "# Epoch 5/100\n",
    "# 370/370 [==============================] - 211s 569ms/step - loss: 0.1487 - cMetric: 0.1486 - maskedMAE: 0.0944 - maskedMAEStart: 0.0926 - val_loss: 0.1476 - val_cMetric: 0.1500 - val_maskedMAE: 0.0949 - val_maskedMAEStart: 0.0876\n",
    "# Epoch 6/100\n",
    "# 370/370 [==============================] - 212s 572ms/step - loss: 0.1359 - cMetric: 0.1359 - maskedMAE: 0.0863 - maskedMAEStart: 0.0845 - val_loss: 0.1297 - val_cMetric: 0.1286 - val_maskedMAE: 0.0814 - val_maskedMAEStart: 0.0846\n",
    "# Epoch 7/100\n",
    "# 370/370 [==============================] - 207s 559ms/step - loss: 0.1263 - cMetric: 0.1266 - maskedMAE: 0.0804 - maskedMAEStart: 0.0797 - val_loss: 0.1268 - val_cMetric: 0.1280 - val_maskedMAE: 0.0815 - val_maskedMAEStart: 0.0776\n",
    "# Epoch 8/100\n",
    "# 370/370 [==============================] - 205s 553ms/step - loss: 0.1182 - cMetric: 0.1182 - maskedMAE: 0.0752 - maskedMAEStart: 0.0746 - val_loss: 0.1246 - val_cMetric: 0.1235 - val_maskedMAE: 0.0787 - val_maskedMAEStart: 0.0885\n",
    "# Epoch 9/100\n",
    "# 370/370 [==============================] - 205s 554ms/step - loss: 0.1128 - cMetric: 0.1127 - maskedMAE: 0.0716 - maskedMAEStart: 0.0714 - val_loss: 0.1132 - val_cMetric: 0.1125 - val_maskedMAE: 0.0710 - val_maskedMAEStart: 0.0736\n",
    "# Epoch 10/100\n",
    "# 370/370 [==============================] - 208s 560ms/step - loss: 0.1073 - cMetric: 0.1074 - maskedMAE: 0.0684 - maskedMAEStart: 0.0674 - val_loss: 0.1135 - val_cMetric: 0.1151 - val_maskedMAE: 0.0731 - val_maskedMAEStart: 0.0764\n",
    "# Epoch 11/100\n",
    "# 370/370 [==============================] - 209s 564ms/step - loss: 0.1024 - cMetric: 0.1023 - maskedMAE: 0.0649 - maskedMAEStart: 0.0644 - val_loss: 0.1134 - val_cMetric: 0.1143 - val_maskedMAE: 0.0725 - val_maskedMAEStart: 0.0714\n",
    "# Epoch 12/100\n",
    "# 370/370 [==============================] - 211s 569ms/step - loss: 0.1011 - cMetric: 0.1011 - maskedMAE: 0.0643 - maskedMAEStart: 0.0632 - val_loss: 0.1098 - val_cMetric: 0.1093 - val_maskedMAE: 0.0693 - val_maskedMAEStart: 0.0707\n",
    "# Epoch 13/100\n",
    "# 370/370 [==============================] - 210s 566ms/step - loss: 0.0960 - cMetric: 0.0960 - maskedMAE: 0.0609 - maskedMAEStart: 0.0604 - val_loss: 0.1046 - val_cMetric: 0.1055 - val_maskedMAE: 0.0672 - val_maskedMAEStart: 0.0703\n",
    "# Epoch 14/100\n",
    "# 370/370 [==============================] - 211s 569ms/step - loss: 0.0933 - cMetric: 0.0931 - maskedMAE: 0.0591 - maskedMAEStart: 0.0582 - val_loss: 0.1001 - val_cMetric: 0.0992 - val_maskedMAE: 0.0628 - val_maskedMAEStart: 0.0651\n",
    "# Epoch 15/100\n",
    "# 370/370 [==============================] - 213s 574ms/step - loss: 0.0904 - cMetric: 0.0904 - maskedMAE: 0.0574 - maskedMAEStart: 0.0573 - val_loss: 0.0990 - val_cMetric: 0.0989 - val_maskedMAE: 0.0629 - val_maskedMAEStart: 0.0658\n",
    "# Epoch 16/100\n",
    "# 370/370 [==============================] - 216s 584ms/step - loss: 0.0883 - cMetric: 0.0882 - maskedMAE: 0.0561 - maskedMAEStart: 0.0554 - val_loss: 0.0989 - val_cMetric: 0.0989 - val_maskedMAE: 0.0629 - val_maskedMAEStart: 0.0675\n",
    "# Epoch 17/100\n",
    "# 370/370 [==============================] - 212s 573ms/step - loss: 0.0864 - cMetric: 0.0864 - maskedMAE: 0.0549 - maskedMAEStart: 0.0544 - val_loss: 0.0998 - val_cMetric: 0.0994 - val_maskedMAE: 0.0632 - val_maskedMAEStart: 0.0625\n",
    "# Epoch 18/100\n",
    "# 370/370 [==============================] - 213s 574ms/step - loss: 0.0837 - cMetric: 0.0837 - maskedMAE: 0.0532 - maskedMAEStart: 0.0521 - val_loss: 0.1015 - val_cMetric: 0.1008 - val_maskedMAE: 0.0643 - val_maskedMAEStart: 0.0650\n",
    "# Epoch 19/100\n",
    "# 370/370 [==============================] - 216s 584ms/step - loss: 0.0831 - cMetric: 0.0837 - maskedMAE: 0.0532 - maskedMAEStart: 0.0518 - val_loss: 0.0978 - val_cMetric: 0.0979 - val_maskedMAE: 0.0617 - val_maskedMAEStart: 0.0652\n",
    "# Epoch 20/100\n",
    "# 370/370 [==============================] - 214s 577ms/step - loss: 0.0806 - cMetric: 0.0806 - maskedMAE: 0.0512 - maskedMAEStart: 0.0502 - val_loss: 0.0979 - val_cMetric: 0.1021 - val_maskedMAE: 0.0649 - val_maskedMAEStart: 0.0642\n",
    "# Epoch 21/100\n",
    "# 370/370 [==============================] - 216s 583ms/step - loss: 0.0795 - cMetric: 0.0796 - maskedMAE: 0.0505 - maskedMAEStart: 0.0497 - val_loss: 0.1028 - val_cMetric: 0.1012 - val_maskedMAE: 0.0644 - val_maskedMAEStart: 0.0639\n",
    "# Epoch 22/100\n",
    "# 370/370 [==============================] - 218s 589ms/step - loss: 0.0774 - cMetric: 0.0774 - maskedMAE: 0.0491 - maskedMAEStart: 0.0487 - val_loss: 0.1084 - val_cMetric: 0.1083 - val_maskedMAE: 0.0686 - val_maskedMAEStart: 0.0709\n",
    "# Epoch 23/100\n",
    "# 370/370 [==============================] - 263s 710ms/step - loss: 0.0754 - cMetric: 0.0753 - maskedMAE: 0.0478 - maskedMAEStart: 0.0479 - val_loss: 0.0985 - val_cMetric: 0.0976 - val_maskedMAE: 0.0617 - val_maskedMAEStart: 0.0620\n",
    "# Epoch 24/100\n",
    "# 370/370 [==============================] - 408s 1s/step - loss: 0.0745 - cMetric: 0.0745 - maskedMAE: 0.0473 - maskedMAEStart: 0.0471 - val_loss: 0.0909 - val_cMetric: 0.0910 - val_maskedMAE: 0.0581 - val_maskedMAEStart: 0.0595\n",
    "# Epoch 25/100\n",
    "# 370/370 [==============================] - 336s 907ms/step - loss: 0.0720 - cMetric: 0.0720 - maskedMAE: 0.0457 - maskedMAEStart: 0.0457 - val_loss: 0.0915 - val_cMetric: 0.0910 - val_maskedMAE: 0.0580 - val_maskedMAEStart: 0.0588\n",
    "# Epoch 26/100\n",
    "# 370/370 [==============================] - 652s 2s/step - loss: 0.0722 - cMetric: 0.0722 - maskedMAE: 0.0459 - maskedMAEStart: 0.0452 - val_loss: 0.0899 - val_cMetric: 0.0885 - val_maskedMAE: 0.0561 - val_maskedMAEStart: 0.0578\n",
    "# Epoch 27/100\n",
    "# 370/370 [==============================] - 664s 2s/step - loss: 0.0698 - cMetric: 0.0698 - maskedMAE: 0.0444 - maskedMAEStart: 0.0442 - val_loss: 0.0901 - val_cMetric: 0.0890 - val_maskedMAE: 0.0565 - val_maskedMAEStart: 0.0602\n",
    "# Epoch 28/100\n",
    "# 370/370 [==============================] - 667s 2s/step - loss: 0.0695 - cMetric: 0.0695 - maskedMAE: 0.0441 - maskedMAEStart: 0.0440 - val_loss: 0.0918 - val_cMetric: 0.0906 - val_maskedMAE: 0.0574 - val_maskedMAEStart: 0.0601\n",
    "# Epoch 29/100\n",
    "# 370/370 [==============================] - 549s 1s/step - loss: 0.0688 - cMetric: 0.0687 - maskedMAE: 0.0436 - maskedMAEStart: 0.0432 - val_loss: 0.0994 - val_cMetric: 0.1001 - val_maskedMAE: 0.0638 - val_maskedMAEStart: 0.0667\n",
    "# Epoch 30/100\n",
    "# 370/370 [==============================] - 205s 553ms/step - loss: 0.0681 - cMetric: 0.0679 - maskedMAE: 0.0431 - maskedMAEStart: 0.0433 - val_loss: 0.0908 - val_cMetric: 0.0892 - val_maskedMAE: 0.0564 - val_maskedMAEStart: 0.0602\n",
    "# Epoch 31/100\n",
    "# 370/370 [==============================] - 204s 551ms/step - loss: 0.0654 - cMetric: 0.0653 - maskedMAE: 0.0415 - maskedMAEStart: 0.0418 - val_loss: 0.0891 - val_cMetric: 0.0887 - val_maskedMAE: 0.0559 - val_maskedMAEStart: 0.0567\n",
    "# Epoch 32/100\n",
    "# 370/370 [==============================] - 207s 559ms/step - loss: 0.0664 - cMetric: 0.0663 - maskedMAE: 0.0421 - maskedMAEStart: 0.0421 - val_loss: 0.0931 - val_cMetric: 0.0924 - val_maskedMAE: 0.0582 - val_maskedMAEStart: 0.0612\n",
    "# Epoch 33/100\n",
    "# 370/370 [==============================] - 204s 552ms/step - loss: 0.0640 - cMetric: 0.0640 - maskedMAE: 0.0406 - maskedMAEStart: 0.0412 - val_loss: 0.0931 - val_cMetric: 0.0946 - val_maskedMAE: 0.0599 - val_maskedMAEStart: 0.0610\n",
    "# Epoch 34/100\n",
    "# 370/370 [==============================] - 204s 551ms/step - loss: 0.0637 - cMetric: 0.0637 - maskedMAE: 0.0404 - maskedMAEStart: 0.0405 - val_loss: 0.0906 - val_cMetric: 0.0943 - val_maskedMAE: 0.0603 - val_maskedMAEStart: 0.0609\n",
    "# Epoch 35/100\n",
    "# 370/370 [==============================] - 204s 551ms/step - loss: 0.0631 - cMetric: 0.0631 - maskedMAE: 0.0401 - maskedMAEStart: 0.0402 - val_loss: 0.0885 - val_cMetric: 0.0891 - val_maskedMAE: 0.0561 - val_maskedMAEStart: 0.0570\n",
    "# Epoch 36/100\n",
    "# 370/370 [==============================] - 204s 552ms/step - loss: 0.0624 - cMetric: 0.0624 - maskedMAE: 0.0396 - maskedMAEStart: 0.0398 - val_loss: 0.0931 - val_cMetric: 0.0919 - val_maskedMAE: 0.0574 - val_maskedMAEStart: 0.0579\n",
    "# Epoch 37/100\n",
    "# 370/370 [==============================] - 206s 555ms/step - loss: 0.0625 - cMetric: 0.0625 - maskedMAE: 0.0396 - maskedMAEStart: 0.0392 - val_loss: 0.0876 - val_cMetric: 0.0865 - val_maskedMAE: 0.0547 - val_maskedMAEStart: 0.0567\n",
    "# Epoch 38/100\n",
    "# 370/370 [==============================] - 210s 568ms/step - loss: 0.0594 - cMetric: 0.0594 - maskedMAE: 0.0377 - maskedMAEStart: 0.0376 - val_loss: 0.0889 - val_cMetric: 0.0885 - val_maskedMAE: 0.0562 - val_maskedMAEStart: 0.0573\n",
    "# Epoch 39/100\n",
    "# 370/370 [==============================] - 209s 565ms/step - loss: 0.0597 - cMetric: 0.0597 - maskedMAE: 0.0379 - maskedMAEStart: 0.0384 - val_loss: 0.0887 - val_cMetric: 0.0881 - val_maskedMAE: 0.0556 - val_maskedMAEStart: 0.0570\n",
    "# Epoch 40/100\n",
    "# 370/370 [==============================] - 209s 565ms/step - loss: 0.0596 - cMetric: 0.0596 - maskedMAE: 0.0378 - maskedMAEStart: 0.0380 - val_loss: 0.0863 - val_cMetric: 0.0860 - val_maskedMAE: 0.0546 - val_maskedMAEStart: 0.0593\n",
    "# Epoch 41/100\n",
    "# 370/370 [==============================] - 207s 560ms/step - loss: 0.0595 - cMetric: 0.0596 - maskedMAE: 0.0378 - maskedMAEStart: 0.0382 - val_loss: 0.0881 - val_cMetric: 0.0876 - val_maskedMAE: 0.0556 - val_maskedMAEStart: 0.0591\n",
    "# Epoch 42/100\n",
    "# 370/370 [==============================] - 207s 560ms/step - loss: 0.0574 - cMetric: 0.0574 - maskedMAE: 0.0364 - maskedMAEStart: 0.0367 - val_loss: 0.0853 - val_cMetric: 0.0846 - val_maskedMAE: 0.0538 - val_maskedMAEStart: 0.0562\n",
    "# Epoch 43/100\n",
    "# 370/370 [==============================] - 209s 565ms/step - loss: 0.0565 - cMetric: 0.0565 - maskedMAE: 0.0359 - maskedMAEStart: 0.0360 - val_loss: 0.0830 - val_cMetric: 0.0827 - val_maskedMAE: 0.0521 - val_maskedMAEStart: 0.0553\n",
    "# Epoch 44/100\n",
    "# 370/370 [==============================] - 209s 563ms/step - loss: 0.0571 - cMetric: 0.0570 - maskedMAE: 0.0362 - maskedMAEStart: 0.0365 - val_loss: 0.0846 - val_cMetric: 0.0848 - val_maskedMAE: 0.0542 - val_maskedMAEStart: 0.0569\n",
    "# Epoch 45/100\n",
    "# 370/370 [==============================] - 214s 578ms/step - loss: 0.0550 - cMetric: 0.0550 - maskedMAE: 0.0349 - maskedMAEStart: 0.0351 - val_loss: 0.0833 - val_cMetric: 0.0843 - val_maskedMAE: 0.0534 - val_maskedMAEStart: 0.0566\n",
    "# Epoch 46/100\n",
    "# 370/370 [==============================] - 214s 578ms/step - loss: 0.0558 - cMetric: 0.0558 - maskedMAE: 0.0354 - maskedMAEStart: 0.0357 - val_loss: 0.0841 - val_cMetric: 0.0841 - val_maskedMAE: 0.0534 - val_maskedMAEStart: 0.0557\n",
    "# Epoch 47/100\n",
    "# 370/370 [==============================] - 213s 574ms/step - loss: 0.0547 - cMetric: 0.0547 - maskedMAE: 0.0347 - maskedMAEStart: 0.0350 - val_loss: 0.0897 - val_cMetric: 0.0889 - val_maskedMAE: 0.0565 - val_maskedMAEStart: 0.0578\n",
    "# Epoch 48/100\n",
    "# 370/370 [==============================] - 214s 578ms/step - loss: 0.0543 - cMetric: 0.0543 - maskedMAE: 0.0344 - maskedMAEStart: 0.0346 - val_loss: 0.0841 - val_cMetric: 0.0861 - val_maskedMAE: 0.0544 - val_maskedMAEStart: 0.0553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52d65e3c584acd8d469b538cd5aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=844.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Validation predictions\n",
    "\n",
    "def evaluateTraces(batch, model, batch_size=1, max_num_waypoints=107):\n",
    "    x_imu, x_wifi, x_beacon, x_wdata, pad_mask, pos_encoding, combined_mask = batch\n",
    "    arr_max_lengths = (pad_mask.shape[1] - (pad_mask!=0)[:,::-1].argmax(1) - 1)[:, 0]\n",
    "    enc_pad_mask, combined_mask, _ = data_generator.createMasks(pad_mask, x_wdata)\n",
    "    for num_batch in range(batch_size):\n",
    "        max_length = arr_max_lengths[num_batch]\n",
    "        output = np.zeros((1, max_num_waypoints, 2))\n",
    "        data_batch = (x_imu, \n",
    "                      x_wifi, \n",
    "                      x_beacon,\n",
    "                      x_wdata, \n",
    "                      enc_pad_mask,\n",
    "                      pos_encoding,\n",
    "                      combined_mask)\n",
    "        predictions = model(data_batch, training=False)\n",
    "    return predictions.numpy().squeeze(0)[:max_length+1, :]\n",
    "    \n",
    "\n",
    "# load_model_name = 'trf_enc_dec_v0.13'\n",
    "# model = models.load_model(PATH_MODELS + 'position_model/' + load_model_name, compile=False)\n",
    "\n",
    "data_generator = DataGenerator(list_train_val_traces, dict_train_traces_paths, batch_size=1, \n",
    "                               dict_floors=dict_training_floor,\n",
    "                               experiment=experiment, shuffle=False, training=False)\n",
    "dict_predictions = {}\n",
    "for i, batch in enumerate(tqdm(data_generator)):\n",
    "    arr_predictions = evaluateTraces(batch, model, batch_size=1)\n",
    "    trace = list_train_val_traces[i]\n",
    "    dict_predictions[trace] = {\n",
    "        'site' : dict_site_floor_traces[trace][0],\n",
    "        'floor' : dict_site_floor_traces[trace][1],\n",
    "        'predictions' : arr_predictions\n",
    "    }\n",
    "    \n",
    "if not os.path.exists(PATH_PREDICTIONS + load_model_name):\n",
    "    os.mkdir(PATH_PREDICTIONS + load_model_name)\n",
    "    \n",
    "with open(f'{PATH_PREDICTIONS}{load_model_name}/val_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_predictions, f)    \n",
    "    \n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0915ffad49f541fc86794d9ca7f0a487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Test predictions\n",
    "\n",
    "# 0. Get floors\n",
    "dict_all_sites = dict_all_sites = experiment.dict_all_sites\n",
    "df_floor = pd.read_csv(PATH_PREDICTIONS + 'floor_model_v0.3' + '/submission.csv')\n",
    "df_floor = df_floor[['site_path_timestamp', 'floor']]\n",
    "\n",
    "df_floor['site'] = df_floor['site_path_timestamp'].\\\n",
    "                                              apply(lambda x: dict_all_sites[x.split('_')[0]])\n",
    "df_floor['path'] = df_floor['site_path_timestamp'].\\\n",
    "                                apply(lambda x: x.split('_')[1])\n",
    "dict_floors_test = {}\n",
    "for i, row in df_floor.iterrows():\n",
    "    dict_floors_test[row['path']] = row['floor']\n",
    "    \n",
    "df_sample_submission = pd.read_csv(PATH_DATA + 'sample_submission.csv')\n",
    "df_sample_submission['site'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                              apply(lambda x: dict_all_sites[x.split('_')[0]])\n",
    "df_sample_submission['path'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                apply(lambda x: x.split('_')[1])\n",
    "df_sample_submission['timestamp'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                apply(lambda x: x.split('_')[2]).astype(np.int64)/1000.0\n",
    "\n",
    "list_columns = ['site_path_timestamp', 'site', 'path', 'timestamp']\n",
    "df_sample_submission = df_sample_submission[list_columns]\n",
    "\n",
    "dict_all_sites_inv = { i: s for i, s in enumerate(dict_all_sites)}\n",
    "dict_test_site_floor_traces = {}\n",
    "for i, row in df_sample_submission.iterrows():\n",
    "    dict_test_site_floor_traces[row['path']] = (dict_all_sites_inv[row['site']], dict_floors_test[row['path']])\n",
    "\n",
    "def evaluateTraces(batch, model, batch_size=1, max_num_waypoints=107):\n",
    "    x_imu, x_wifi, x_beacon, x_wdata, pad_mask, pos_encoding, pos_encoding_timestamp = batch\n",
    "    arr_max_lengths = (pad_mask.shape[1] - (pad_mask!=0)[:,::-1].argmax(1) - 1)[:, 0]\n",
    "    enc_pad_mask, combined_mask, _ = data_generator.createMasks(pad_mask, x_wdata)\n",
    "    for num_batch in range(batch_size):\n",
    "        max_length = arr_max_lengths[num_batch]\n",
    "        output = np.zeros((1, max_num_waypoints, 2))\n",
    "        data_batch = (x_imu, \n",
    "                      x_wifi,\n",
    "                      x_beacon,\n",
    "                      x_wdata, \n",
    "                      enc_pad_mask,\n",
    "                      pos_encoding,\n",
    "                      combined_mask)\n",
    "        predictions = model(data_batch, training=False)\n",
    "    return predictions.numpy().squeeze(0)[:max_length+1, :], arr_max_lengths\n",
    "    \n",
    "\n",
    "# load_model_name = 'trf_enc_dec_v0.13'\n",
    "# model = models.load_model(PATH_MODELS + 'position_model/' + load_model_name, compile=False)\n",
    "\n",
    "test_data_generator = DataGenerator(list_test_traces, dict_test_traces_paths, batch_size=1, \n",
    "                               dict_floors=dict_floors_test,\n",
    "                               experiment=experiment, shuffle=False, training=False)\n",
    "\n",
    "dict_test_predictions = {}\n",
    "for i, batch in enumerate(tqdm(test_data_generator)):\n",
    "    arr_predictions, arr_max_lengths = evaluateTraces(batch, model, batch_size=1)\n",
    "    trace = list_test_traces[i]    \n",
    "    dict_test_predictions[trace] = {\n",
    "        'site' : dict_test_site_floor_traces[trace][0],\n",
    "        'floor' : dict_test_site_floor_traces[trace][1],\n",
    "        'predictions' : arr_predictions\n",
    "    }\n",
    "    \n",
    "if not os.path.exists(PATH_PREDICTIONS + load_model_name):\n",
    "    os.mkdir(PATH_PREDICTIONS + load_model_name)\n",
    "\n",
    "with open(f'{PATH_PREDICTIONS}{load_model_name}/predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_test_predictions, f)     \n",
    "    \n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
