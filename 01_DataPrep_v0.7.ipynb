{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 0. Libraries\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "from functools import reduce\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "import scipy.signal as signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "from PIL import Image\n",
    "import plotly.graph_objs as go\n",
    "import cv2\n",
    "\n",
    "np.random.seed(12)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 1. Global Variables & Paths\n",
    "\n",
    "# Paths \n",
    "\n",
    "PATH_DATA = '../../01_Data/'\n",
    "path_data_train = PATH_DATA + 'train/'\n",
    "path_data_test = PATH_DATA + 'test/'\n",
    "path_metadata = PATH_DATA + 'metadata/'\n",
    "\n",
    "\n",
    "# Global Variables\n",
    "\n",
    "DICT_FLOOR_MAP = {'1F' :  0, '2F' : 1, '3F' : 2, '4F' : 3, '5F' : 4, \n",
    "                     '6F' : 5, '7F' : 6, '8F' : 7, '9F' : 8,\n",
    "                     'B'  : -1, 'B1' : -1, 'B2' : -2, 'B3' : -3, \n",
    "                     'BF' : -1, 'BM' : -1, \n",
    "                     'F1' : 0, 'F2' : 1, 'F3' : 2, 'F4' : 3, 'F5' : 4, \n",
    "                     'F6' : 5, 'F7' : 6, 'F8' : 7, 'F9' : 8, 'F10': 9,\n",
    "                     'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3, 'L5' : 4, \n",
    "                     'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8, 'L10': 9, \n",
    "                     'L11': 10,\n",
    "                     'G'  : 0, 'LG1': 0, 'LG2': 1, 'LM' : 0, 'M'  : 0, \n",
    "                     'P1' : 0, 'P2' : 1,}\n",
    "\n",
    "SEQ_LEN_IMU = 128\n",
    "SEQ_LEN_WIFI = 150\n",
    "SEQ_LEN_BEACON = 100\n",
    "\n",
    "NETWORK_SIZE = 550\n",
    "VERSION = '010'\n",
    "\n",
    "list_train_paths = glob.glob(path_data_train + '*/*/*')\n",
    "list_test_paths = glob.glob(path_data_test + '*')\n",
    "list_metadata_paths = glob.glob(path_metadata + '*')\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 2. Classes\n",
    "\n",
    "# copy from https://github.com/location-competition/indoor-location-competition-20/blob/master/io_f.py\n",
    "@dataclass\n",
    "class ReadData:\n",
    "    acce: np.ndarray\n",
    "    acce_uncali: np.ndarray\n",
    "    gyro: np.ndarray\n",
    "    gyro_uncali: np.ndarray\n",
    "    magn: np.ndarray\n",
    "    magn_uncali: np.ndarray\n",
    "    ahrs: np.ndarray\n",
    "    wifi: np.ndarray\n",
    "    ibeacon: np.ndarray\n",
    "    waypoint: np.ndarray\n",
    "        \n",
    "    \n",
    "class Experiment(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__ = kwargs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 3. Functions\n",
    "\n",
    "def getMeanStd(df, cols):\n",
    "    return {col: df[col].mean() for col in cols}, {col: df[col].std() for col in cols}\n",
    "\n",
    "\n",
    "def getCategories(df, cols):\n",
    "    cats_d = {}\n",
    "    for col in cols:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            print(f'{col} already categorized')\n",
    "        else:\n",
    "            df[col] = pd.Categorical(df[col])\n",
    "        cats_d[col] = df[col].cat.categories.values\n",
    "    return cats_d\n",
    "\n",
    "\n",
    "# https://github.com/location-competition/indoor-location-competition-20/blob/master/io_f.py\n",
    "def readDataFile(path_input):\n",
    "    acce = []\n",
    "    acce_uncali = []\n",
    "    gyro = []\n",
    "    gyro_uncali = []\n",
    "    magn = []\n",
    "    magn_uncali = []\n",
    "    ahrs = []\n",
    "    wifi = []\n",
    "    ibeacon = []\n",
    "    waypoint = []\n",
    "\n",
    "    with open(path_input, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line_data in lines:\n",
    "        line_data = line_data.strip()\n",
    "        if not line_data or line_data[0] == '#':\n",
    "            continue\n",
    "\n",
    "        line_data = line_data.split('\\t')\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER':\n",
    "            acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n",
    "            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE':\n",
    "            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n",
    "            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n",
    "            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n",
    "            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_ROTATION_VECTOR':\n",
    "            if len(line_data)>=5:\n",
    "                ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WIFI':\n",
    "            sys_ts = line_data[0]\n",
    "            ssid = line_data[2]\n",
    "            bssid = line_data[3]\n",
    "            rssi = line_data[4]\n",
    "            lastseen_ts = line_data[6]\n",
    "            wifi_data = [sys_ts, ssid, bssid, rssi, lastseen_ts]\n",
    "            wifi.append(wifi_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_BEACON':\n",
    "            ts = line_data[0]\n",
    "            uuid = line_data[2]\n",
    "            major = line_data[3]\n",
    "            minor = line_data[4]\n",
    "            rssi = line_data[6]\n",
    "            lastts = line_data[-1]\n",
    "            ibeacon_data = [ts, '_'.join([uuid, major, minor]), rssi, lastts]\n",
    "            ibeacon.append(ibeacon_data)\n",
    "            continue\n",
    "\n",
    "        if line_data[1] == 'TYPE_WAYPOINT':\n",
    "            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n",
    "\n",
    "    acce = np.array(acce)\n",
    "    acce_uncali = np.array(acce_uncali)\n",
    "    gyro = np.array(gyro)\n",
    "    gyro_uncali = np.array(gyro_uncali)\n",
    "    magn = np.array(magn)\n",
    "    magn_uncali = np.array(magn_uncali)\n",
    "    ahrs = np.array(ahrs)\n",
    "    wifi = np.array(wifi)\n",
    "    ibeacon = np.array(ibeacon)\n",
    "    waypoint = np.array(waypoint)\n",
    "    \n",
    "    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)\n",
    "\n",
    "    \n",
    "# Prepare IMU data\n",
    "    \n",
    "def getImuData(sample_file):\n",
    "    if sample_file.acce.shape[0]==0 or sample_file.acce_uncali.shape[0]==0 or sample_file.gyro.shape[0]==0 or \\\n",
    "       sample_file.gyro_uncali.shape[0]==0 or sample_file.magn.shape[0]==0 or sample_file.magn_uncali.shape[0]==0 or \\\n",
    "       sample_file.ahrs.shape[0]==0:\n",
    "        df_imu = pd.DataFrame()\n",
    "    else:\n",
    "        df_acce = pd.DataFrame({\n",
    "            'timestamp' : sample_file.acce[:, 0],\n",
    "            'acce_x' : sample_file.acce[:, 1],\n",
    "            'acce_y' : sample_file.acce[:, 2],\n",
    "            'acce_z' : sample_file.acce[:, 3]\n",
    "        })\n",
    "        df_acce_uncali = pd.DataFrame({\n",
    "            'timestamp' : sample_file.acce_uncali[:, 0],\n",
    "            'acce_uncali_x' : sample_file.acce_uncali[:, 1],\n",
    "            'acce_uncali_y' : sample_file.acce_uncali[:, 2],\n",
    "            'acce_uncali_z' : sample_file.acce_uncali[:, 3]\n",
    "        })\n",
    "        df_gyro = pd.DataFrame({\n",
    "            'timestamp' : sample_file.gyro[:, 0],\n",
    "            'gyro_x' : sample_file.gyro[:, 1],\n",
    "            'gyro_y' : sample_file.gyro[:, 2],\n",
    "            'gyro_z' : sample_file.gyro[:, 3]\n",
    "        })\n",
    "        df_gyro_uncali = pd.DataFrame({\n",
    "            'timestamp' : sample_file.gyro_uncali[:, 0],\n",
    "            'gyro_uncali_x' : sample_file.gyro_uncali[:, 1],\n",
    "            'gyro_uncali_y' : sample_file.gyro_uncali[:, 2],\n",
    "            'gyro_uncali_z' : sample_file.gyro_uncali[:, 3]\n",
    "        })\n",
    "        df_magn = pd.DataFrame({\n",
    "            'timestamp' : sample_file.magn[:, 0],\n",
    "            'magn_x' : sample_file.magn[:, 1],\n",
    "            'magn_y' : sample_file.magn[:, 2],\n",
    "            'magn_z' : sample_file.magn[:, 3]\n",
    "        })\n",
    "        df_magn_uncali = pd.DataFrame({\n",
    "            'timestamp' : sample_file.magn_uncali[:, 0],\n",
    "            'magn_uncali_x' : sample_file.magn_uncali[:, 1],\n",
    "            'magn_uncali_y' : sample_file.magn_uncali[:, 2],\n",
    "            'magn_uncali_z' : sample_file.magn_uncali[:, 3]\n",
    "        })\n",
    "        df_ahrs = pd.DataFrame({\n",
    "            'timestamp' : sample_file.ahrs[:, 0],\n",
    "            'ahrs_x' : sample_file.ahrs[:, 1],\n",
    "            'ahrs_y' : sample_file.ahrs[:, 2],\n",
    "            'ahrs_z' : sample_file.ahrs[:, 3]\n",
    "        })\n",
    "\n",
    "        list_df_imu = [df_acce, df_acce_uncali, df_gyro, df_gyro_uncali, df_magn, df_magn_uncali, df_ahrs]\n",
    "        df_imu = reduce(lambda left, right: pd.merge(left, right, on=['timestamp'],\n",
    "                                                      how='inner'), list_df_imu)\n",
    "        df_imu = df_imu.sort_values(['timestamp']).reset_index(drop=True)\n",
    "        df_imu['timestamp'] = df_imu['timestamp']#/1000.0\n",
    "        df_imu[['acce_x', 'acce_y', 'acce_z']] = df_imu[['acce_x', 'acce_y', 'acce_z']].cumsum()\n",
    "        df_imu[['acce_uncali_x', 'acce_uncali_y', 'acce_uncali_z']] = df_imu[['acce_uncali_x', 'acce_uncali_y', 'acce_uncali_z']].cumsum()\n",
    "        df_imu[['gyro_x', 'gyro_y', 'gyro_z']] = df_imu[['gyro_x', 'gyro_y', 'gyro_z']].cumsum()\n",
    "        df_imu[['gyro_uncali_x', 'gyro_uncali_y', 'gyro_uncali_z']] = df_imu[['gyro_uncali_x', 'gyro_uncali_y', 'gyro_uncali_z']].cumsum()\n",
    "        \n",
    "    return df_imu\n",
    "\n",
    "\n",
    "def getWifiData(sample_file):\n",
    "    if sample_file.wifi.shape[0]>0:\n",
    "        df_wifi = pd.DataFrame(sample_file.wifi)\n",
    "        df_wifi.columns = ['timestamp', 'ssid', 'bssid', 'rssi', 'last_seen_timestamp']\n",
    "        df_wifi['timestamp'] = df_wifi['timestamp'].astype(np.int64)\n",
    "        df_wifi['last_seen_timestamp'] = df_wifi['last_seen_timestamp'].astype(np.int64)\n",
    "        df_wifi = df_wifi.sort_values(['timestamp']).reset_index(drop=True)\n",
    "        df_wifi['timestamp'] = df_wifi['timestamp']#/1000.0\n",
    "        df_wifi['last_seen_timestamp'] = df_wifi['last_seen_timestamp']#/1000.0\n",
    "    else:\n",
    "        df_wifi = pd.DataFrame()\n",
    "    \n",
    "    return df_wifi\n",
    "\n",
    "\n",
    "def getBeaconData(sample_file, test=False):\n",
    "    if sample_file.ibeacon.shape[0]>0:\n",
    "        df_beacon = pd.DataFrame(sample_file.ibeacon)\n",
    "        if not test:\n",
    "            df_beacon = df_beacon.iloc[:, :-1]\n",
    "            df_beacon.columns = ['timestamp', 'uuid', 'rssi']\n",
    "            df_beacon['timestamp'] = df_beacon['timestamp'].astype(np.int64)\n",
    "        else:\n",
    "            df_beacon.columns = ['timestamp', 'uuid', 'rssi', 'last_timestamp']\n",
    "            df_beacon['timestamp'] = df_beacon['timestamp'].astype(np.int64)\n",
    "            df_beacon['last_timestamp'] = df_beacon['last_timestamp'].astype(np.int64)\n",
    "            # df_beacon = df_beacon.drop(['last_timestamp'], axis=1)\n",
    "    else:\n",
    "        df_beacon = pd.DataFrame()\n",
    "\n",
    "    return df_beacon\n",
    "\n",
    "\n",
    "def getWaypointData(sample_file):\n",
    "    df_waypoint = pd.DataFrame(sample_file.waypoint)\n",
    "    df_waypoint.columns = ['timestamp', 'waypoint_x','waypoint_y']\n",
    "    df_waypoint['timestamp'] = df_waypoint['timestamp']#/1000.0\n",
    "    df_waypoint['ts_diff_start'] = df_waypoint['timestamp'].diff().fillna(0.).cumsum()\n",
    "    df_waypoint['ts_diff_last'] = (df_waypoint['timestamp'] - df_waypoint['timestamp'].shift(periods=1)).fillna(df_waypoint['timestamp']-df_waypoint['timestamp'])\n",
    "    # df_waypoint['ts_diff_sec'] = df_waypoint['ts_diff'].apply(lambda x: x.total_seconds()).fillna(0)\n",
    "    # df_waypoint['ts_diff_milisec'] = df_waypoint['ts_diff_sec'] * 1000\n",
    "\n",
    "    return df_waypoint\n",
    "\n",
    "\n",
    "## Sequences\n",
    "\n",
    "def getSequencesImu(df, df_w, window_mean=5):\n",
    "    list_seqs = []\n",
    "    timestamps = df_w['timestamp'].values\n",
    "    min_ts, max_ts = df_w['timestamp'].min(), df_w['timestamp'].max()\n",
    "    if df.shape[0]>=1:\n",
    "        df = df[(df['timestamp'] >= min_ts) & (df['timestamp'] <= max_ts)].reset_index(drop=True)\n",
    "        for i in range(len(timestamps)):\n",
    "            if i==0:\n",
    "                arr_values = np.zeros((SEQ_LEN_IMU, 24))\n",
    "            else:\n",
    "                ts_actual, ts_next = timestamps[i-1], timestamps[i]\n",
    "                df_tmp_ = df[(df['timestamp'] >= ts_actual) & (df['timestamp'] < ts_next)].reset_index(drop=True)\n",
    "                df_tmp_ = df_tmp_.groupby(np.arange(len(df_tmp_)) // window_mean).mean()\n",
    "                df_tmp_['ts_diff_w'] = np.abs(ts_actual - df_tmp_['timestamp'])\n",
    "                df_tmp_['ts_diff_w0'] = np.abs(min_ts - df_tmp_['timestamp'])\n",
    "                df_tmp_['timestamp'] = pd.to_datetime(df_tmp_['timestamp']/1000.0, unit='s')\n",
    "                series_ts_diff = (df_tmp_['timestamp'] - df_tmp_['timestamp'].shift(periods=1)).fillna(df_tmp_['timestamp']-df_tmp_['timestamp'])\n",
    "                df_tmp_['ts_diff_last'] = series_ts_diff.apply(lambda x: x.total_seconds())\n",
    "                arr_values = df_tmp_.values[-SEQ_LEN_IMU:, 1:].astype(np.float32)\n",
    "\n",
    "            padt = (0, (SEQ_LEN_IMU-arr_values.shape[0]))\n",
    "            arr_values = np.pad(arr_values, (padt, (0, 0)), constant_values=(0))\n",
    "            list_seqs.append(np.expand_dims(arr_values, 0))\n",
    "        \n",
    "    return list_seqs\n",
    "\n",
    "\n",
    "def getSequencesWifi(df, df_w, dict_unique_ssid, dict_unique_bssid):\n",
    "    list_seqs = []\n",
    "    timestamps = df_w['timestamp'].values\n",
    "    min_ts, max_ts = df_w['timestamp'].min(), df_w['timestamp'].max()\n",
    "    if df.shape[0]>=1:\n",
    "        df = df[df['timestamp'] <= max_ts].reset_index(drop=True)\n",
    "        for i in range(len(timestamps)):\n",
    "            if i==0:\n",
    "                ts_actual = timestamps[i]\n",
    "                df_tmp_ = df[df['last_seen_timestamp'] < ts_actual].reset_index(drop=True)\n",
    "            else:\n",
    "                ts_actual, ts_next = timestamps[i-1], timestamps[i]\n",
    "                df_tmp_ = df[(df['last_seen_timestamp'] >= ts_actual) & (df['last_seen_timestamp'] < ts_next)].reset_index(drop=True)\n",
    "\n",
    "            # Most relevant signals\n",
    "            df_tmp_['rssi'] = df_tmp_['rssi'].astype(np.int32) / 100.0\n",
    "            df_tmp_ = df_tmp_.sort_values(['rssi'], ascending=False).reset_index(drop=True)\n",
    "            df_tmp_['ts_diff_sec'] = df_tmp_['timestamp'] - df_tmp_['last_seen_timestamp']\n",
    "            df_tmp_['ts_diff_w'] = ts_actual - df_tmp_['last_seen_timestamp']\n",
    "            df_tmp_['ts_diff_w0'] = np.abs(min_ts - df_tmp_['last_seen_timestamp'])\n",
    "            df_tmp_['ssid'] = df_tmp_['ssid'].apply(lambda x: dict_unique_ssid[x] if x in dict_unique_ssid else dict_unique_ssid['<NA>'])\n",
    "            df_tmp_['bssid'] = df_tmp_['bssid'].apply(lambda x: dict_unique_bssid[x] if x in dict_unique_bssid else dict_unique_bssid['<NA>'])\n",
    "            df_tmp_ = df_tmp_.drop(['last_seen_timestamp'], axis=1)\n",
    "            arr_values = df_tmp_.values[:SEQ_LEN_WIFI, 1:].astype(np.float32)\n",
    "            padt = (0, (SEQ_LEN_WIFI-arr_values.shape[0]))\n",
    "            arr_values = np.pad(arr_values, (padt, (0, 0)), constant_values=(0))\n",
    "            list_seqs.append(np.expand_dims(arr_values, 0))\n",
    "    else:\n",
    "        arr_values = np.zeros((SEQ_LEN_BEACON, 7))\n",
    "        list_seqs = [np.expand_dims(arr_values, 0) for _ in range(timestamps.shape[0])]\n",
    "        \n",
    "    return list_seqs\n",
    "\n",
    "\n",
    "def getSequencesBeacon(df, df_w, dict_unique_uuid):\n",
    "    list_seqs = []\n",
    "    timestamps = df_w['timestamp'].values\n",
    "    min_ts, max_ts = df_w['timestamp'].min(), df_w['timestamp'].max()\n",
    "    if df.shape[0]>=1:\n",
    "        df = df[df['timestamp'] <= max_ts].reset_index(drop=True)\n",
    "        for i in range(len(timestamps)):\n",
    "            if i==0:\n",
    "                ts_actual = timestamps[i]\n",
    "                df_tmp_ = df[df['timestamp'] <= ts_actual].reset_index(drop=True)\n",
    "            else:\n",
    "                ts_actual, ts_next = timestamps[i-1], timestamps[i]\n",
    "                df_tmp_ = df[(df['timestamp'] >= ts_actual) & (df['timestamp'] < ts_next)].reset_index(drop=True)\n",
    "            \n",
    "            # Most relevant signals\n",
    "            df_tmp_['rssi'] = df_tmp_['rssi'].astype(np.int32) / 100.0\n",
    "            df_tmp_ = df_tmp_.sort_values(['rssi'], ascending=False).reset_index(drop=True)\n",
    "            df_tmp_['ts_diff_w'] = ts_actual - df_tmp_['timestamp']\n",
    "            df_tmp_['ts_diff_w0'] = np.abs(min_ts - df_tmp_['timestamp'])\n",
    "            df_tmp_['uuid'] = df_tmp_['uuid'].apply(lambda x: dict_unique_uuid[x] if x in dict_unique_uuid else dict_unique_uuid['<NA>'])\n",
    "            arr_values = df_tmp_.values[:SEQ_LEN_BEACON, 1:].astype(np.float32)\n",
    "            padt = (0, (SEQ_LEN_BEACON-arr_values.shape[0]))\n",
    "            arr_values = np.pad(arr_values, (padt, (0, 0)), constant_values=(0))\n",
    "            list_seqs.append(np.expand_dims(arr_values, 0))\n",
    "    else:\n",
    "        arr_values = np.zeros((SEQ_LEN_BEACON, 4))\n",
    "        list_seqs = [np.expand_dims(arr_values, 0) for _ in range(timestamps.shape[0])]\n",
    "        \n",
    "    return list_seqs\n",
    "\n",
    "\n",
    "def getWaypointDataSampleSubmission(df_ss, path):\n",
    "    df_ss = df_ss[df_ss['path']==path].reset_index(drop=True)\n",
    "    df_ss['ts_diff_start'] = df_ss['timestamp'].diff().fillna(0.).cumsum()\n",
    "    df_ss['ts_diff_last'] = (df_ss['timestamp'] - df_ss['timestamp'].shift(periods=1)).fillna(df_ss['timestamp']-df_ss['timestamp'])\n",
    "    return df_ss\n",
    "\n",
    "# AllSequences for floor prediction \n",
    "\n",
    "def getAllFloorPrediction(df_w, df_imu_, df_wifi_, df_beacon_, window_mean=20):\n",
    "    min_ts, max_ts = df_w['timestamp'].min(), df_w['timestamp'].max()\n",
    "    # Filter\n",
    "    df_imu_ = df_imu_[(df_imu_['timestamp'] >= min_ts) & (df_imu_['timestamp'] <= max_ts)].reset_index(drop=True)\n",
    "    df_wifi_ = df_wifi_[(df_wifi_['timestamp'] >= min_ts) & (df_wifi_['timestamp'] <= max_ts)].reset_index(drop=True)\n",
    "    \n",
    "    # Imu\n",
    "    df_imu_ = df_imu_[['timestamp', 'magn_x', 'magn_y', 'magn_z', 'magn_uncali_x', 'magn_uncali_y', 'magn_uncali_z']]\n",
    "    df_imu_ = df_imu_.groupby(np.arange(len(df_imu_)) // window_mean).mean().reset_index(drop=True)\n",
    "    imu_data = df_imu_.values[:100, 1:]\n",
    "    padt = (0, (100-imu_data.shape[0]))\n",
    "    imu_data = np.pad(imu_data, (padt, (0, 0)), constant_values=(0))\n",
    "    \n",
    "    # Wifi\n",
    "    df_wifi_['rssi'] = df_wifi_['rssi'].astype(np.int32) / 100.0\n",
    "    df_wifi_ = df_wifi_.sort_values(['rssi'], ascending=False).reset_index(drop=True)\n",
    "    df_wifi_ = df_wifi_[['timestamp', 'ssid', 'bssid', 'rssi']]\n",
    "    df_wifi_['ssid'] = df_wifi_['ssid'].apply(lambda x: dict_unique_ssid[x] if x in dict_unique_ssid else dict_unique_ssid['<NA>'])\n",
    "    df_wifi_['bssid'] = df_wifi_['bssid'].apply(lambda x: dict_unique_bssid[x] if x in dict_unique_bssid else dict_unique_bssid['<NA>'])\n",
    "    wifi_data = df_wifi_.values[:NETWORK_SIZE, 1:]\n",
    "    padt = (0, (NETWORK_SIZE-wifi_data.shape[0]))\n",
    "    wifi_data = np.pad(wifi_data, (padt, (0, 0)), constant_values=(0))\n",
    "    \n",
    "    # Beacon\n",
    "    if len(df_beacon_)>0:\n",
    "        df_beacon_ = df_beacon_[['timestamp', 'uuid', 'rssi']]\n",
    "        df_beacon_ = df_beacon_[(df_beacon_['timestamp'] >= min_ts) & (df_beacon_['timestamp'] <= max_ts)].reset_index(drop=True)\n",
    "        df_beacon_['rssi'] = df_beacon_['rssi'].astype(np.int32) / 100.0\n",
    "        df_beacon_ = df_beacon_.sort_values(['rssi'], ascending=False).reset_index(drop=True)\n",
    "        df_beacon_['uuid'] = df_beacon_['uuid'].apply(lambda x: dict_unique_uuid[x] if x in dict_unique_uuid else dict_unique_uuid['<NA>'])\n",
    "        beacon_data = df_beacon_.values[:NETWORK_SIZE, 1:]\n",
    "        padt = (0, (NETWORK_SIZE-beacon_data.shape[0]))\n",
    "        beacon_data = np.pad(beacon_data, (padt, (0, 0)), constant_values=(0))\n",
    "    else:\n",
    "        beacon_data = np.zeros((NETWORK_SIZE, 2))\n",
    "    return imu_data, wifi_data, beacon_data\n",
    "\n",
    "\n",
    "def getTestGapTimeStamp(df_beacon_, df_wifi_):\n",
    "    if len(df_beacon_)>=1:\n",
    "        gap = df_beacon_['last_timestamp'] - df_beacon_['timestamp']\n",
    "        assert gap.unique().shape[0]==1\n",
    "        gap = gap.values[0]\n",
    "    else:\n",
    "        wifi_groups = df_wifi_.groupby('timestamp')  \n",
    "        gap = (wifi_groups['last_seen_timestamp'].max().astype(np.int64) - wifi_groups['timestamp'].max().astype(int)).max()\n",
    "        gap = gap\n",
    "    return gap \n",
    "    \n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dcd8b95d5b4c82a7adabe961528bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26925.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8c97b2ad6055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mnum_processors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# with parallel_backend('threading', n_jobs=num_processors):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_processors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadCategories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_train_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfeat_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 4. Get Wifi & Beacon Categories\n",
    "\n",
    "list_unique_ssid = []\n",
    "list_unique_bssid = []\n",
    "list_unique_uuid = []\n",
    "list_features = []\n",
    "\n",
    "\n",
    "def readCategories(file):\n",
    "    sample_file = readDataFile(path_input=file)\n",
    "    df_wifi = getWifiData(sample_file)\n",
    "    df_beacon = getBeaconData(sample_file)\n",
    "    if df_wifi.shape[0] > 0:\n",
    "        v0, v1 = df_wifi['ssid'].unique(), df_wifi['bssid'].unique()\n",
    "    else:\n",
    "        v0, v1 = np.array([]), np.array([])\n",
    "    if df_beacon.shape[0] > 0:\n",
    "        v2 = df_beacon['uuid'].unique()\n",
    "    else:\n",
    "        v2 = np.array([])\n",
    "    return (v0, v1, v2)\n",
    "    \n",
    "num_processors = 16\n",
    "# with parallel_backend('threading', n_jobs=num_processors):\n",
    "results = Parallel(n_jobs=num_processors)(delayed(readCategories)(file) for file in tqdm(list_train_paths))\n",
    "    \n",
    "for feat_ in tqdm(results):\n",
    "    ssid, bssid, uuid  = feat_\n",
    "    list_unique_ssid.append(ssid)\n",
    "    list_unique_bssid.append(bssid)\n",
    "    list_unique_uuid.append(uuid)\n",
    "    \n",
    "list_unique_ssid = np.unique(np.concatenate(list_unique_ssid))\n",
    "list_unique_bssid = np.unique(np.concatenate(list_unique_bssid))\n",
    "list_unique_uuid = np.unique(np.concatenate(list_unique_uuid))\n",
    "\n",
    "dict_unique_ssid = {value : i+1 for i, value in enumerate(list_unique_ssid)}\n",
    "dict_unique_bssid = {value : i+1 for i, value in enumerate(list_unique_bssid)}\n",
    "dict_unique_uuid = {value : i+1 for i, value in enumerate(list_unique_uuid)}\n",
    "\n",
    "dict_unique_ssid['<NA>'] = 0\n",
    "dict_unique_bssid['<NA>'] = 0\n",
    "dict_unique_uuid['<NA>'] = 0\n",
    "\n",
    "list_all_sites = set([path.split(\"metadata\\\\\")[-1] for path in list_metadata_paths])\n",
    "dict_all_sites = {site_ : i for i, site_ in enumerate(list_all_sites)}\n",
    "\n",
    "# #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 5. Get num waypoints distributions on both sets\n",
    "\n",
    "# df_sample_submission = pd.read_csv(PATH_DATA + 'sample_submission.csv')\n",
    "# df_sample_submission['site'] = df_sample_submission['site_path_timestamp'].\\\n",
    "#                                               apply(lambda x: x.split('_'))\n",
    "# df_sample_submission['path'] = df_sample_submission['site_path_timestamp'].\\\n",
    "#                                 apply(lambda x: x.split('_')[1])\n",
    "# df_sample_submission['timestamp'] = df_sample_submission['site_path_timestamp'].\\\n",
    "#                                 apply(lambda x: x.split('_')[2]).astype(np.int64)/1000.0\n",
    "\n",
    "# list_columns = ['site_path_timestamp', 'site', 'path', 'timestamp']\n",
    "# df_sample_submission = df_sample_submission[list_columns]\n",
    "\n",
    "# list_num_waypoints = []\n",
    "# for file_ in tqdm(list_test_paths):\n",
    "#     # Load data\n",
    "#     sample_file = readDataFile(path_input=file_)\n",
    "#     path = file_.split('\\\\')[-1].replace('.txt', '')\n",
    "#     ## Preprocess data\n",
    "#     df = getWaypointDataSampleSubmission(df_sample_submission, path)\n",
    "#     if df.shape[0]>=1:\n",
    "#         list_num_waypoints.append(df.shape[0])\n",
    "        \n",
    "# unique, counts = np.unique(list_num_waypoints, return_counts=True)\n",
    "# print(pd.Series(list_num_waypoints).describe())\n",
    "\n",
    "#Train\n",
    "# count    26925.000000\n",
    "# mean         6.190641\n",
    "# std          5.814579\n",
    "# min          1.000000\n",
    "# 25%          3.000000\n",
    "# 50%          4.000000\n",
    "# 75%          7.000000\n",
    "# max        107.000000\n",
    "\n",
    "#Test\n",
    "# count    626.000000\n",
    "# mean      16.186901\n",
    "# std        9.459229\n",
    "# min        6.000000\n",
    "# 25%       10.000000\n",
    "# 50%       14.000000\n",
    "# 75%       18.750000\n",
    "# max      107.000000\n",
    "\n",
    "# with open(f'{path_generated_data}dict_training_waypoints_{VERSION}.pkl', 'rb') as f:\n",
    "#     dict_training_waypoints = pickle.load(f)\n",
    "\n",
    "# list_num_waypoints = []\n",
    "# for site in dict_training_waypoints:\n",
    "#     for floor in dict_training_waypoints[site]:\n",
    "#         assert len(dict_training_waypoints[site][floor]['x'])==len(dict_training_waypoints[site][floor]['y'])\n",
    "#         if len(dict_training_waypoints[site][floor]['x']) >= 1:\n",
    "#             list_num_waypoints.append(len(dict_training_waypoints[site][floor]['x']))   \n",
    "        \n",
    "# pd.Series(list_num_waypoints).describe()\n",
    "\n",
    "# count     967.000000\n",
    "# mean      172.371251\n",
    "# std       247.254857\n",
    "# min         3.000000\n",
    "# 25%        43.000000\n",
    "# 50%        81.000000\n",
    "# 75%       168.500000\n",
    "# max      2027.000000\n",
    "# dtype: float64\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 5. Get All training points for each site\n",
    "# list_all_floors = np.unique(list(DICT_FLOOR_MAP.values()))\n",
    "# list_all_sites = set([dict_all_sites[path.split(\"metadata\\\\\")[-1]] for path in list_metadata_paths])\n",
    "\n",
    "# dict_training_waypoints = {}\n",
    "# for site in list_all_sites:\n",
    "#     dict_training_waypoints[site] = {}\n",
    "#     for floor in list_all_floors:\n",
    "#         dict_training_waypoints[site][floor] = {\n",
    "#             'x' : [],\n",
    "#             'y' : []\n",
    "#         }\n",
    "\n",
    "# for train_file_ in tqdm(list_train_paths):\n",
    "#     sample_file = readDataFile(path_input=train_file_)\n",
    "#     floor = DICT_FLOOR_MAP[train_file_.split('\\\\')[-2]]\n",
    "#     site = dict_all_sites[train_file_.split('\\\\')[1]]\n",
    "#     df_waypoint = getWaypointData(sample_file)\n",
    "#     if df_waypoint.shape[0]>=1:\n",
    "#         df_waypoint['floor'] = floor\n",
    "#         df_waypoint['site'] = site\n",
    "#         dict_training_waypoints[site][floor]['x'].extend(df_waypoint['waypoint_x'].values/100.)\n",
    "#         dict_training_waypoints[site][floor]['y'].extend(df_waypoint['waypoint_y'].values/100.)\n",
    "        \n",
    "# with open(f'{path_generated_data}/dict_training_waypoints_{VERSION}.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_training_waypoints, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Preparing train files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a034ca76044c8f9c28002e46dbb0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26925.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 6. Build Train sequences\n",
    "# ~1h\n",
    "# We dont have records before starting point on training so its useless trying to predict it.\n",
    "# For each path point(f, x, y) we will use the data from later timestamps to predict the actual waypoint.\n",
    "\n",
    "path_output_data = PATH_DATA + 'GeneratedData/v0.3/train/'\n",
    "list_all_sites = set([path.split(\"metadata\\\\\")[-1] for path in list_metadata_paths])\n",
    "list_traces_train_filtered, list_floors_train_filtered = [], []\n",
    "print('->Preparing train files...')\n",
    "for train_file_ in tqdm(list_train_paths):\n",
    "    # Load data\n",
    "    sample_file = readDataFile(path_input=train_file_)\n",
    "    floor = DICT_FLOOR_MAP[train_file_.split('\\\\')[-2]]\n",
    "    site = dict_all_sites[train_file_.split('\\\\')[1]]\n",
    "    path = train_file_.split('\\\\')[-1].replace('.txt', '')\n",
    "    ## Preprocess data\n",
    "    df_waypoint = getWaypointData(sample_file)\n",
    "    # df_waypoint = df_waypoint.iloc[1:, :]\n",
    "    if df_waypoint.shape[0]>=1:\n",
    "        df_waypoint['floor'] = floor\n",
    "        df_waypoint['site'] = site\n",
    "        df_imu = getImuData(sample_file)\n",
    "        df_wifi = getWifiData(sample_file)\n",
    "        if df_imu.shape[0] >= 1 and df_wifi.shape[0] >= 1:\n",
    "            df_beacon = getBeaconData(sample_file)\n",
    "            \n",
    "            ## Prepare sequences\n",
    "            # Waypoint\n",
    "            data_waypoint = df_waypoint[['site', 'ts_diff_start', 'ts_diff_last']]\n",
    "            data_waypoint_y = df_waypoint[['floor', 'waypoint_x', 'waypoint_y']]\n",
    "            # Imu\n",
    "            seq_imu = getSequencesImu(df_imu, df_waypoint, window_mean=8)#0.02sec->0.18sec\n",
    "            seq_imu = np.concatenate(seq_imu)\n",
    "            # Wifi\n",
    "            seq_wifi = getSequencesWifi(df_wifi, df_waypoint, dict_unique_ssid, dict_unique_bssid)\n",
    "            seq_wifi = np.concatenate(seq_wifi)\n",
    "            # Beacon\n",
    "            seq_beacon = getSequencesBeacon(df_beacon, df_waypoint, dict_unique_uuid)\n",
    "            seq_beacon = np.concatenate(seq_beacon)\n",
    "            \n",
    "            # Final assertions\n",
    "            assert seq_imu.shape==(df_waypoint.shape[0], SEQ_LEN_IMU, 24)\n",
    "            assert seq_wifi.shape==(df_waypoint.shape[0], SEQ_LEN_WIFI, 6)\n",
    "            assert seq_beacon.shape==(df_waypoint.shape[0], SEQ_LEN_BEACON, 4)\n",
    "            assert data_waypoint.shape==(df_waypoint.shape[0], 3)\n",
    "            assert data_waypoint_y.shape==(df_waypoint.shape[0], 3)\n",
    "            list_traces_train_filtered.append(path)\n",
    "            list_floors_train_filtered.append(floor)\n",
    "            \n",
    "            # Get condensed data \n",
    "            all_imu, all_wifi, all_beacon = getAllFloorPrediction(df_waypoint, df_imu, df_wifi, df_beacon, window_mean=20)\n",
    "            assert all_imu.shape == (100, 6)\n",
    "            assert all_wifi.shape == (NETWORK_SIZE, 3)\n",
    "            assert all_beacon.shape == (NETWORK_SIZE, 2)\n",
    "            ## Save data\n",
    "            # Seq\n",
    "\n",
    "            path_out = path_output_data + path\n",
    "            if not os.path.exists(path_out):\n",
    "                os.mkdir(path_out)\n",
    "            np.save(f'{path_out}/seq_imu.npy', seq_imu)\n",
    "            np.save(f'{path_out}/seq_wifi.npy', seq_wifi)\n",
    "            np.save(f'{path_out}/seq_beacon.npy', seq_beacon)\n",
    "            np.save(f'{path_out}/waypoint_data.npy', data_waypoint)\n",
    "            np.save(f'{path_out}/waypoint_predict.npy', data_waypoint_y)\n",
    "            # All\n",
    "            np.save(f'{path_out}/all_imu.npy', all_imu)\n",
    "            np.save(f'{path_out}/all_wifi.npy', all_wifi)\n",
    "            np.save(f'{path_out}/all_beacon.npy', all_beacon)\n",
    "            \n",
    "# print(seq_imu.shape, seq_wifi.shape, seq_beacon.shape, data_waypoint.shape, data_waypoint_y.shape)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Preparing test files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73da87c2d6c430092aa8f248ac11150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 7. Build Test sequences\n",
    "\n",
    "path_output_data = PATH_DATA + 'GeneratedData/v0.3/test/'\n",
    "list_all_sites = set([path.split(\"metadata\\\\\")[-1] for path in list_metadata_paths])\n",
    "list_paths_test_filtered = []\n",
    "print('->Preparing test files...')\n",
    "\n",
    "# 6.1Parse sample submission\n",
    "df_sample_submission = pd.read_csv(PATH_DATA + 'sample_submission.csv')\n",
    "df_sample_submission['site'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                              apply(lambda x: dict_all_sites[x.split('_')[0]])\n",
    "df_sample_submission['path'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                apply(lambda x: x.split('_')[1])\n",
    "df_sample_submission['timestamp'] = df_sample_submission['site_path_timestamp'].\\\n",
    "                                apply(lambda x: x.split('_')[2]).astype(np.int64)\n",
    "\n",
    "list_columns = ['site_path_timestamp', 'site', 'path', 'timestamp']\n",
    "df_sample_submission = df_sample_submission[list_columns]\n",
    "\n",
    "# 6.2 Get Sequences\n",
    "for i, test_file_ in enumerate(tqdm(list_test_paths)):\n",
    "    # Load data\n",
    "    sample_file = readDataFile(path_input=test_file_)\n",
    "    path = test_file_.split('\\\\')[-1].replace('.txt', '')\n",
    "    ## Preprocess data\n",
    "    df_waypoint = getWaypointDataSampleSubmission(df_sample_submission, path)\n",
    "    df_beacon = getBeaconData(sample_file, test=True)\n",
    "    df_wifi = getWifiData(sample_file)\n",
    "    gap = getTestGapTimeStamp(df_beacon, df_wifi)\n",
    "    df_waypoint['timestamp'] = df_waypoint['timestamp'] + gap\n",
    "    df_wifi['timestamp'] = df_wifi['timestamp'] + gap\n",
    "    assert np.sum((df_waypoint['timestamp']>=1e12) & (df_waypoint['timestamp']<=1e15))==len(df_waypoint)\n",
    "    if len(df_beacon)>=1:\n",
    "        df_beacon['timestamp'] = df_beacon['timestamp'] + gap\n",
    "        df_beacon = df_beacon.drop(['last_timestamp'], axis=1)\n",
    "    df_imu = getImuData(sample_file)\n",
    "    df_imu['timestamp'] = df_imu['timestamp'] + gap\n",
    "    if df_imu.shape[0] >= 1 and df_wifi.shape[0] >= 1:\n",
    "        ## Prepare sequences\n",
    "        # Waypoint\n",
    "        data_waypoint = df_waypoint[['site', 'ts_diff_start', 'ts_diff_last']]\n",
    "        # Imu\n",
    "        seq_imu = getSequencesImu(df_imu, df_waypoint, window_mean=8)#0.02sec->0.18sec\n",
    "        seq_imu = np.concatenate(seq_imu)\n",
    "        # Wifi\n",
    "        seq_wifi = getSequencesWifi(df_wifi, df_waypoint, dict_unique_ssid, dict_unique_bssid)\n",
    "        seq_wifi = np.concatenate(seq_wifi)\n",
    "        # Beacon\n",
    "        seq_beacon = getSequencesBeacon(df_beacon, df_waypoint, dict_unique_uuid)\n",
    "        seq_beacon = np.concatenate(seq_beacon)\n",
    "        # Final assertions\n",
    "        assert seq_imu.shape==(df_waypoint.shape[0], SEQ_LEN_IMU, 24)\n",
    "        assert seq_wifi.shape==(df_waypoint.shape[0], SEQ_LEN_WIFI, 6)\n",
    "        assert seq_beacon.shape==(df_waypoint.shape[0], SEQ_LEN_BEACON, 4)\n",
    "        assert data_waypoint.shape==(df_waypoint.shape[0], 3)\n",
    "\n",
    "        # Get condensed data \n",
    "        all_imu, all_wifi, all_beacon = getAllFloorPrediction(df_waypoint, df_imu, df_wifi, df_beacon, window_mean=20)\n",
    "        ## Save data\n",
    "        # Seq\n",
    "        \n",
    "        path_out = path_output_data + path\n",
    "        if not os.path.exists(path_out):\n",
    "            os.mkdir(path_out)\n",
    "        np.save(f'{path_out}/seq_imu.npy', seq_imu)\n",
    "        np.save(f'{path_out}/seq_wifi.npy', seq_wifi)\n",
    "        np.save(f'{path_out}/seq_beacon.npy', seq_beacon)\n",
    "        np.save(f'{path_out}/waypoint_data.npy', data_waypoint)\n",
    "        # All\n",
    "        np.save(f'{path_out}/all_imu.npy', all_imu)\n",
    "        np.save(f'{path_out}/all_wifi.npy', all_wifi)\n",
    "        np.save(f'{path_out}/all_beacon.npy', all_beacon)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da67c23a0b634f67b0dce65f6a356d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=24464.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# 8. Get stats\n",
    "\n",
    "path_generated_data = PATH_DATA + 'GeneratedData/v0.3/'\n",
    "list_paths_generated_data_train = glob.glob(path_generated_data + 'train/*')\n",
    "list_paths_generated_data_test = glob.glob(path_generated_data + 'test/*')\n",
    "list_mean_imu, list_std_imu = [], []\n",
    "list_mean_w, list_std_w = [], []\n",
    "list_mean_wifi, list_std_wifi = [], []\n",
    "list_mean_beacon, list_std_beacon = [], []\n",
    "for i, train_file_ in enumerate(tqdm(list_paths_generated_data_train)):\n",
    "    # Load data\n",
    "    data_imu = np.expand_dims(np.load(train_file_ + '/seq_imu.npy'), 0)\n",
    "    data_waypoint = np.expand_dims(np.load(train_file_ + '/waypoint_data.npy'), 0)\n",
    "    data_wifi = np.expand_dims(np.load(train_file_ + '/seq_wifi.npy'), 0)\n",
    "    data_beacon = np.expand_dims(np.load(train_file_ + '/seq_beacon.npy'), 0)\n",
    "    mean_ = data_imu.mean(axis=(1, 2)).squeeze()\n",
    "    std_ = data_imu.std(axis=(1, 2)).squeeze()\n",
    "    list_mean_imu.append(mean_)\n",
    "    list_std_imu.append(std_)\n",
    "    mean_ = data_waypoint[:, :, 1:].mean(axis=1).squeeze()\n",
    "    std_ = data_waypoint[:, :, 1:].std(axis=1).squeeze()\n",
    "    list_mean_w.append(mean_)\n",
    "    list_std_w.append(std_)\n",
    "    mean_ = data_wifi[:, :, :, 2:].mean(axis=(1, 2)).squeeze()\n",
    "    std_ = data_wifi[:, :, :, 2:].std(axis=(1, 2)).squeeze()\n",
    "    list_mean_wifi.append(mean_)\n",
    "    list_std_wifi.append(std_)\n",
    "    mean_ = data_beacon[:, :, :, 1:].mean(axis=(1, 2)).squeeze()\n",
    "    std_ = data_beacon[:, :, :, 1:].std(axis=(1, 2)).squeeze()\n",
    "    list_mean_beacon.append(mean_)\n",
    "    list_std_beacon.append(std_)\n",
    "    # print(data_wifibeacon[:, :, :, 6].min(), data_wifibeacon[:, :, :, 6].max())\n",
    "\n",
    "mean_imu = np.mean(np.asarray([d for d in list_mean_imu]), axis=0) \n",
    "std_imu = np.mean(np.asarray([d for d in list_std_imu]), axis=0)\n",
    "mean_w = np.mean(np.asarray([d for d in list_mean_w]), axis=0) \n",
    "std_w = np.mean(np.asarray([d for d in list_std_w]), axis=0)\n",
    "mean_wifi = np.mean(np.asarray([d for d in list_mean_wifi]), axis=0) \n",
    "std_wifi = np.mean(np.asarray([d for d in list_std_wifi]), axis=0)\n",
    "mean_beacon = np.mean(np.asarray([d for d in list_mean_beacon]), axis=0) \n",
    "std_beacon = np.mean(np.asarray([d for d in list_std_beacon]), axis=0)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 8. Get all waypoints\n",
    "\n",
    "# path_generated_data = PATH_DATA + 'GeneratedData/'\n",
    "# list_paths_generated_data_train = glob.glob(path_generated_data + 'train/*')\n",
    "# list_paths_generated_data_test = glob.glob(path_generated_data + 'test/*')\n",
    "# list_mean_imu, list_std_imu = [], []\n",
    "# list_mean_wx, list_std_wx, list_mean_wy, list_std_wy = [], [], [], []\n",
    "# df_all_waypoints = pd.DataFrame()\n",
    "# for train_file_ in tqdm(list_train_paths):\n",
    "#     # Load data\n",
    "#     site = train_file_.split('\\\\')[1]\n",
    "#     sample_file = readDataFile(path_input=train_file_)\n",
    "#     df_waypoint = getWaypointData(sample_file)\n",
    "#     df_waypoint['site'] = site\n",
    "#     df_all_waypoints = pd.concat([df_all_waypoints, df_waypoint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# df_all_waypoints = df_all_waypoints.sort_values(['site', 'timestamp']). reset_index(drop=True)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 9. Save experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    mean_imu=mean_imu,\n",
    "    std_imu=std_imu,\n",
    "    mean_w=mean_w,\n",
    "    std_w=std_w,\n",
    "    mean_wifi=mean_wifi,\n",
    "    std_wifi=std_wifi,\n",
    "    mean_beacon=mean_beacon,\n",
    "    std_beacon=std_beacon,\n",
    "    list_traces_train_filtered=list_traces_train_filtered,\n",
    "    list_floors_train_filtered=list(set(list_floors_train_filtered)),\n",
    "    dict_all_sites=dict_all_sites,\n",
    "    dict_unique_uuid=dict_unique_uuid,\n",
    "    dict_unique_bssid=dict_unique_bssid,\n",
    "    dict_unique_ssid=dict_unique_ssid,\n",
    "    dict_floor_map=DICT_FLOOR_MAP,\n",
    "    seq_len_imu=SEQ_LEN_IMU,\n",
    "    version=VERSION\n",
    ")\n",
    "\n",
    "with open(f'{path_generated_data}/Experiment_{VERSION}.pkl', 'wb') as f:\n",
    "    pickle.dump(experiment, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
